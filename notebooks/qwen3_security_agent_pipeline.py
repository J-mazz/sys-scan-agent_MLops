import json

notebook_content = {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Qwen3 Security Agent: End-to-End Pipeline\n",
    "**SFT + GRPO + GGUF Export + Chunking**\n",
    "\n",
    "This notebook implements the complete training lifecycle:\n",
    "1.  **Stage 1 (SFT):** Teaches the model the strict JSON schema requirements.\n",
    "2.  **Stage 2 (GRPO):** Optimizes reasoning accuracy and severity scoring.\n",
    "3.  **Production:** Merges adapters, exports GGUF, and **chunks the binary** for PyPi distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Environment & Configuration\n",
    "import os\n",
    "import torch\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Mount Drive\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# 2. Install Dependencies (L4/A100 Optimized)\n",
    "!pip install --upgrade -qqq uv\n",
    "!uv pip install -qqq --upgrade unsloth vllm torchvision bitsandbytes xformers\n",
    "!uv pip install --no-deps trl==0.22.2 transformers==4.56.2\n",
    "\n",
    "# 3. Configuration (Paths & Settings)\n",
    "class Config:\n",
    "    base_model = \"unsloth/Qwen2.5-3B-Instruct\"\n",
    "    project_name = \"Qwen_Security_Agent\"\n",
    "    \n",
    "    # --- PATHS (All in Drive) ---\n",
    "    base_path = f\"/content/drive/MyDrive/{project_name}\"\n",
    "    sft_output = os.path.join(base_path, \"sft_adapter\")\n",
    "    grpo_output = os.path.join(base_path, \"grpo_adapter\")\n",
    "    export_dir = os.path.join(base_path, \"production_models\")\n",
    "    cache_dir = os.path.join(base_path, \"cache\")\n",
    "    \n",
    "cfg = Config()\n",
    "\n",
    "for path in [cfg.sft_output, cfg.grpo_output, cfg.export_dir, cfg.cache_dir]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Configured. All artifacts will be saved to: {cfg.base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Stage 1: SFT (Format Alignment)\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# 1. Load Base Model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = cfg.base_model,\n",
    "    max_seq_length = 4096,\n",
    "    load_in_4bit = True,\n",
    "    max_lora_rank = 64,\n",
    "    cache_dir = cfg.cache_dir,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model, r=64, lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    use_gradient_checkpointing=\"unsloth\", random_state=3407,\n",
    ")\n",
    "\n",
    "# 2. Formatting\n",
    "SYSTEM_PROMPT = \"\"\"You are a Senior Cyber-Security Architect.\n",
    "Analyze the system scan finding and output a JSON object with the following fields:\n",
    "- risk_score: 0-100 (Integer)\n",
    "- severity: \"critical\", \"high\", \"medium\", \"low\", \"informational\"\n",
    "- rationale: A technical explanation.\n",
    "- mitigation_steps: A list of actions.\n",
    "\n",
    "Output ONLY the JSON object.\"\"\"\n",
    "\n",
    "def sft_formatting(examples):\n",
    "    texts = []\n",
    "    for i in range(len(examples[\"title\"])):\n",
    "        user = {k: examples[k][i] for k in [\"title\", \"description\", \"metadata\"]}\n",
    "        gt = {\n",
    "            \"risk_score\": int(examples[\"risk_score\"][i]),\n",
    "            \"severity\": examples[\"severity\"][i],\n",
    "            \"rationale\": examples[\"rationale\"][i],\n",
    "            \"mitigation_steps\": examples.get(\"mitigation_steps\", [])[i]\n",
    "        }\n",
    "        msgs = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(user, indent=2)},\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps(gt)}\n",
    "        ]\n",
    "        texts.append(tokenizer.apply_chat_template(msgs, tokenize=False))\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = load_dataset(\"jmazz/sys-scan_synthetic_dataset_v2\", split=\"train[:500]\", cache_dir=cfg.cache_dir)\n",
    "\n",
    "# 3. Train\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    formatting_func=sft_formatting,\n",
    "    processing_class=tokenizer,\n",
    "    args=SFTConfig(\n",
    "        output_dir=cfg.sft_output,\n",
    "        max_seq_length=4096,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    ")\n",
    "trainer.train()\n",
    "model.save_lora(cfg.sft_output)\n",
    "print(f\"‚úÖ SFT Saved to: {cfg.sft_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3. Transition: Merge & Reload (vLLM)\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "import gc\n",
    "from unsloth import PatchFastRL\n",
    "\n",
    "# 1. Clean Memory\n",
    "del model, trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 2. Merge SFT Adapter\n",
    "print(\"üîÑ Merging SFT Adapter...\")\n",
    "base = AutoModelForCausalLM.from_pretrained(cfg.base_model, device_map=\"cpu\", dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base, cfg.sft_output)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save to local temporary path for vLLM (faster than Drive)\n",
    "temp_merge_path = \"/content/merged_sft_temp\"\n",
    "model.save_pretrained(temp_merge_path)\n",
    "tokenizer.save_pretrained(temp_merge_path)\n",
    "del base, model\n",
    "gc.collect()\n",
    "\n",
    "# 3. Reload with vLLM + GRPO\n",
    "print(\"üöÄ Reloading with vLLM acceleration...\")\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=temp_merge_path,\n",
    "    max_seq_length=4096,\n",
    "    load_in_4bit=False, # Bfloat16 for GRPO stability\n",
    "    fast_inference=True,\n",
    "    gpu_memory_utilization=0.6,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model, r=64, lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    use_gradient_checkpointing=\"unsloth\", random_state=3407\n",
    ")\n",
    "print(\"‚úÖ Ready for GRPO.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4. Stage 2: GRPO (Reasoning Optimization)\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import re\n",
    "\n",
    "# Rewards\n",
    "def soft_extract(text, key, type_fn):\n",
    "    m = re.search(r'\"' + re.escape(key) + r'\"\\s*:\\s*\"?([^\",\\}\\n]+)\"?', text)\n",
    "    if m: return type_fn(re.sub(r\"[^0-9]\", \"\", m.group(1)) if type_fn == int else m.group(1))\n",
    "    return None\n",
    "\n",
    "def json_validity_reward(completions, **kwargs):\n",
    "    rewards = []\n",
    "    for c in completions:\n",
    "        try:\n",
    "            json.loads(c[0][\"content\"])\n",
    "            rewards.append(1.0)\n",
    "        except:\n",
    "            rewards.append(0.5 if \"{\" in c[0][\"content\"] else 0.0)\n",
    "    return rewards\n",
    "\n",
    "def risk_accuracy_reward(completions, answer, **kwargs):\n",
    "    rewards = []\n",
    "    for c, gt in zip(completions, answer):\n",
    "        pred = soft_extract(c[0][\"content\"], \"risk_score\", int)\n",
    "        gt_score = json.loads(gt).get(\"risk_score\", 0)\n",
    "        if pred is not None:\n",
    "            diff = abs(pred - gt_score)\n",
    "            rewards.append(max(0.0, 1.0 - (diff / 100.0)))\n",
    "        else: rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "def severity_accuracy_reward(completions, answer, **kwargs):\n",
    "    rewards = []\n",
    "    for c, gt in zip(completions, answer):\n",
    "        pred = soft_extract(c[0][\"content\"], \"severity\", str)\n",
    "        gt_sev = json.loads(gt).get(\"severity\", \"info\").lower()\n",
    "        rewards.append(1.0 if pred and pred.lower() == gt_sev else 0.0)\n",
    "    return rewards\n",
    "\n",
    "# Data Prep\n",
    "def format_grpo(x):\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": json.dumps({k: x[k] for k in [\"title\", \"description\", \"metadata\"]}, indent=2)}\n",
    "        ],\n",
    "        \"answer\": json.dumps({\"risk_score\": x.get(\"risk_score\", 0), \"severity\": x.get(\"severity\", \"info\")})\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\"jmazz/sys-scan_synthetic_dataset_v2\", split=\"train\", cache_dir=cfg.cache_dir).map(format_grpo)\n",
    "\n",
    "# Training\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[json_
