import json

# Configuration
HF_REPO_ID = "jmazz/sys-scan_synthetic_dataset_v2"
# CHANGED: Points to the folder containing config.json and .safetensors shards
MODEL_ID = "/content/drive/MyDrive/Qwen3_analyst"

notebook_content = {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Sys-Scan Logic Engine: The 'Pass 2' Pipeline\n",
    "**Logic-First Training: SFT Primer + Causal GRPO (Drive Safetensors)**\n",
    "\n",
    "This pipeline trains your **Drive-hosted model** (`Qwen3_analyst`).\n",
    "**Targeting:** Merged Safetensors (ignoring GGUF/Shards in the same dir).\n",
    "\n",
    "**Strategy:**\n",
    "1.  **Phase 1 (SFT):** Structural Priming (`sft_pass2.jsonl`).\n",
    "2.  **Phase 2 (GRPO):** Reasoning Optimization (`grpo_pass2.jsonl`).\n",
    "3.  **Phase 3 (Export):** Merges the 'Logic Adapter' and chunks it for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Environment Setup\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Mount Drive (CRITICAL: This allows us to access your model)\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# 2. Install Dependencies\n",
    "!pip install --upgrade -qqq uv\n",
    "!uv pip install -qqq --upgrade unsloth vllm torchvision bitsandbytes xformers trl peft accelerate datasets\n",
    "\n",
    "# 3. Login to Hugging Face (Required for pulling the DATASETS)\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "try:\n",
    "    login(token=userdata.get('HF_TOKEN'))\n",
    "except:\n",
    "    login() # Interactive login if secret not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Load Safetensors from Google Drive\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None # Auto detection\n",
    "load_in_4bit = True\n",
    "\n",
    "print(f\"üìÇ Loading model from Drive: {MODEL_ID}...\")\n",
    "print(\"   - Targeting: config.json + model-*.safetensors\")\n",
    "print(\"   - Ignoring: .gguf and other shards\")\n",
    "\n",
    "# Unsloth defaults to loading 'config.json' and the associated safetensors map\n",
    "# It will NOT load the GGUF unless we explicitly used 'model_name=...gguf'\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    f"    model_name = \"{MODEL_ID}\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    gpu_memory_utilization = 0.7,\n",
    ")\n",
    "print(\"‚úÖ Drive Model (Safetensors) Loaded Successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3. Phase 1: Structural Primer (SFT)\n",
    "# Goal: Teach the model the SYNTAX of thinking (<think> ... </think> <answer> ... </answer>)\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# 1. Load SFT Dataset (Broad Syntax Coverage)\n",
    f"dataset_sft = load_dataset(\"{HF_REPO_ID}\", data_files=\"sft_pass2.jsonl\", split=\"train\")\n",
    "\n",
    "# 2. The Logic Protocol Prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert security analyst.\n",
    "1. Analyze the finding in a <think> block.\n",
    "2. Output the classification in a valid JSON <answer> block.\"\"\"\n",
    "\n",
    "# 3. Formatter: Maps 'rationale' -> <think>\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for i in range(len(examples[\"title\"]):\n",
    "        # Input: Telemetry\n",
    "        input_payload = {\n",
    "            \"title\": examples[\"title\"][i],\n",
    "            \"description\": examples[\"description\"][i],\n",
    "            \"metadata\": examples[\"metadata\"][i],\n",
    "            \"category\": examples[\"category\"][i]\n",
    "        }\n",
    "        # Thought: The Causal Trace (from synthetic factory)\n",
    "        thought = examples[\"rationale\"][i]\n",
    "        # Answer: Structured Classification\n",
    "        answer = {\n",
    "            \"risk_score\": int(examples[\"risk_score\"][i]),\n",
    "            \"severity\": examples[\"severity\"][i]\n",
    "        }\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(input_payload, indent=2)},\n",
    "            {\"role\": \"assistant\", \"content\": f\"<think>\\n{thought}\\n</think>\\n<answer>\\n{json.dumps(answer)}\\n</answer>\"}\n",
    "        ]\n",
    "        texts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False))\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# 4. Apply LoRA Adapters for SFT\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 64,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    ")\n",
    "\n",
    "# 5. Train SFT\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset_sft,\n",
    "    dataset_text_field = \"text\",\n",
    "    formatting_func = formatting_prompts_func,\n",
    "    args = SFTConfig(\n",
    "        output_dir = \"sft_primer_adapter\",\n",
    "        max_steps = 250,        # Short run: we only need to learn the XML tags\n",
    "        learning_rate = 2e-4,\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 10,\n",
    "    ),\n",
    "    processing_class = tokenizer,\n",
    ")\n",
    "\n",
    "print(\"üß† Starting Phase 1: Structural Priming...\")\n",
    "trainer.train()\n",
    "model.save_lora(\"sft_primer_adapter\")\n",
    "print(\"‚úÖ SFT Complete. Model now speaks <think>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4. Phase 2: Reasoning Optimization (GRPO)\n",
    "# Goal: Reward the model for using 'because', 'however', and getting the math right.\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import re\n",
    "\n",
    "# 1. Reload Model from Drive (Clean Slate + SFT Adapter)\n",
    "del model, trainer\n",
    "import gc; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"üìÇ Reloading Drive model (Safetensors) for GRPO...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    f"    model_name = \"{MODEL_ID}\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    "    gpu_memory_utilization = 0.6, # Reduced for GRPO rollouts\n",
    ")\n",
    "model.load_lora(\"sft_primer_adapter\")\n",
    "\n",
    "# 2. Define The Rewards\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward strict XML tag usage.\"\"\"\n",
    "    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n",
    "    return [1.0 if re.match(pattern, c[0][\"content\"], re.DOTALL) else 0.0 for c in completions]\n",
    "\n",
    "def reasoning_structure_reward(completions, **kwargs):\n",
    "    \"\"\"Reward 'Cognitive Anchors' that signal depth.\"\"\"\n",
    "    anchors = [\"**because**\", \"**however**\", \"**implies**\", \"**therefore**\", \"impact\", \"mitigated\"]\n",
    "    rewards = []\n",
    "    for c in completions:\n",
    "        content = c[0][\"content\"]\n",
    "        match = re.search(r\"<think>(.*?)</think>\", content, re.DOTALL)\n",
    "        if not match:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        thought = match.group(1).lower()\n",
    "        # Score: +0.2 for each unique anchor used\n",
    "        score = sum(0.2 for a in anchors if a.lower() in thought)\n",
    "        rewards.append(min(1.0, score))\n",
    "    return rewards\n",
    "\n",
    "def risk_accuracy_reward(completions, answer, **kwargs):\n",
    "    \"\"\"Reward correct integer risk scoring.\"\"\"\n",
    "    rewards = []\n",
    "    for c, gt in zip(completions, answer):\n",
    "        try:\n",
    "            # Extract JSON from <answer>\n",
    "            ans_match = re.search(r\"<answer>(.*?)</answer>\", c[0][\"content\"], re.DOTALL)\n",
    "            if not ans_match: \n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "                \n",
    "            pred_json = json.loads(ans_match.group(1))\n",
    "            gt_json = json.loads(gt)\n",
    "            \n",
    "            diff = abs(pred_json.get(\"risk_score\", 0) - gt_json.get(\"risk_score\", 0))\n",
    "            if diff <= 5: rewards.append(1.0)\n",
    "            elif diff <= 15: rewards.append(0.5)\n",
    "            else: rewards.append(0.0)\n",
    "        except:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "# 3. Load GRPO Dataset (Deep Complexity)\n",
    f"dataset_grpo = load_dataset(\"{HF_REPO_ID}\", data_files=\"grpo_pass2.jsonl\", split=\"train\")\n",
    "\n",
    "# Data Prep for GRPO\n",
    "def format_grpo_input(x):\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": json.dumps({\n",
    "                \"title\": x[\"title\"], \n",
    "                \"description\": x[\"description\"], \n",
    "                \"metadata\": x[\"metadata\"],\n",
    "                \"category\": x[\"category\"]\n",
    "            }, indent=2)}\n",
    "        ],\n",
    "        \"answer\": json.dumps({\"risk_score\": x[\"risk_score\"], \"severity\": x[\"severity\"]})\n",
    "    }\n",
    "dataset_grpo = dataset_grpo.map(format_grpo_input)\n",
    "\n",
    "# 4. Train GRPO\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [format_reward, reasoning_structure_reward, risk_accuracy_reward],\n",
    "    args = GRPOConfig(\n",
    "        output_dir = \"final_logic_adapter\",\n",
    "        learning_rate = 5e-6,         # Very low LR for RL alignment\n",
    "        num_generations = 4,\n",
    "        max_prompt_length = 1024,\n",
    "        max_completion_length = 1024,\n",
    "        max_steps = 300,\n",
    "        use_vllm = True,\n",
    "        bf16 = True,\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    "    train_dataset = dataset_grpo,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting Phase 2: Optimizing the Logic...\")\n",
    "trainer.train()\n",
    "model.save_lora(\"final_logic_adapter\")\n",
    "print(\"‚úÖ GRPO Complete. Model is now a Reasoning Engine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5. Export & Chunking\n",
    "# Merge the final adapter and export to GGUF\n",
    "if os.path.exists(\"final_logic_adapter\"):\n",
    "    model.save_pretrained_merged(\"merged_model\", tokenizer, save_method=\"merged_16bit\")\n",
    "    model.push_to_hub_merged(\"jmazz/sys-scan-logic-v1\", tokenizer, save_method=\"merged_16bit\", token=userdata.get('HF_TOKEN'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

with open("Qwen3_Analyst_Pass2.ipynb", "w") as f:
    json.dump(notebook_content, f, indent=1)