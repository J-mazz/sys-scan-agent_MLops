{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f459697",
   "metadata": {},
   "source": [
    "## 0) Environment Setup (Nightly Unsloth + Qwen3-VL-2B)\n",
    "\n",
    "**Why:** Nightly Unsloth builds ship the latest kernels, RLHF utilities, and Qwen3 fixes that we rely on for reproducible LoRA fine-tuning.\n",
    "\n",
    "**Data posture:** All training samples are streamed from Hugging Face (`jmazz/sys-scan-linux-synthetic`) to avoid copying the 600k example corpus into VS Code.\n",
    "\n",
    "**Safety:** This notebook keeps the workflow purely defensive—reasoning about synthetic detections and correlations while never generating exploit guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d81fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run configuration and deterministic defaults\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RunConfig:\n",
    "    base_model_id: str = \"Qwen/Qwen3-VL-2B-Thinking\"\n",
    "    quantized_repo_hint: str = \"unsloth/Qwen3-VL-2B-Thinking-GGUF\"\n",
    "    dataset_repo_id: str = \"jmazz/sys-scan-linux-synthetic\"\n",
    "    dataset_split: str = \"train\"\n",
    "    max_seq_length: int = 2048\n",
    "    lora_rank: int = 16\n",
    "    load_in_4bit: bool = True\n",
    "    seed: int = 424242\n",
    "    shuffle_buffer: int = 8192\n",
    "    max_train_steps: int = 1200\n",
    "    per_device_batch_size: int = 1\n",
    "    grad_accum_steps: int = 8\n",
    "    eval_sample_count: int = 256\n",
    "    max_findings_in_prompt: int = 3\n",
    "    max_correlations_in_prompt: int = 2\n",
    "    grpo_max_steps: int = 400\n",
    "    grpo_per_device_batch_size: int = 1\n",
    "    grpo_grad_accum_steps: int = 4\n",
    "    grpo_num_generations: int = 4\n",
    "    grpo_beta: float = 0.04\n",
    "    grpo_learning_rate: float = 5e-6\n",
    "    artifact_root: Path = Path(\"outputs/qwen3_vl_finetune\")\n",
    "    timestamp: str = field(default_factory=lambda: datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "    @property\n",
    "    def sft_dir(self) -> Path:\n",
    "        return self.artifact_root / \"sft\"\n",
    "\n",
    "    @property\n",
    "    def grpo_dir(self) -> Path:\n",
    "        return self.artifact_root / \"grpo\"\n",
    "\n",
    "    @property\n",
    "    def log_dir(self) -> Path:\n",
    "        return self.artifact_root / \"logs\"\n",
    "\n",
    "    def ensure_dirs(self) -> None:\n",
    "        for path in (self.artifact_root, self.sft_dir, self.grpo_dir, self.log_dir):\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "config = RunConfig()\n",
    "config.ensure_dirs()\n",
    "\n",
    "random.seed(config.seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(config.seed)\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a senior security analytics engineer. Reason step-by-step about synthetic \"\n",
    "    \"host and network telemetry, explain causal links, and only emit JSON that conforms \"\n",
    "    \"to the sys-scan ground_truth schema (version ground_truth_v1). Always prefer \"\n",
    "    \"defensive mitigations, never offensive guidance.\"\n",
    ")\n",
    "\n",
    "print(f\"Base model: {config.base_model_id}\")\n",
    "print(f\"Dataset: {config.dataset_repo_id}::{config.dataset_split} (streaming)\")\n",
    "print(f\"Artifacts: {config.artifact_root.resolve()}\")\n",
    "print(f\"Quantized deployment hint: {config.quantized_repo_hint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip uninstall -y unsloth unsloth_zoo > /dev/null 2>&1 || true\n",
    "!pip install --upgrade pip\n",
    "!pip install --no-cache-dir git+https://github.com/unslothai/unsloth.git\n",
    "!pip install --no-cache-dir git+https://github.com/unslothai/unsloth.git#subdirectory=unsloth_zoo\n",
    "!pip install --upgrade datasets accelerate bitsandbytes huggingface_hub transformers trl vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2205282",
   "metadata": {},
   "source": [
    "## 1) Model and Precision Setup\n",
    "\n",
    "**Why:** Qwen3-VL-2B-Thinking pairs well with a LoRA adapter for text-only security reasoning while keeping the door open for future multimodal telemetry.\n",
    "\n",
    "**Precision:** Load in 4-bit (QLoRA) to stay within 16–24 GiB VRAM and compute in bf16 when available.\n",
    "\n",
    "**Reproducibility:** Seed PyTorch/CUDA before instantiating the model so we can replay the run deterministically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import textwrap\n",
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "def set_torch_seed(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise EnvironmentError(\"CUDA device not detected — attach an NVIDIA GPU before continuing.\")\n",
    "device_props = torch.cuda.get_device_properties(0)\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "total_vram_gib = device_props.total_memory / 1024**3\n",
    "\n",
    "if \"L4\" in device_name:\n",
    "    gpu_memory_utilization = 0.70\n",
    "elif \"T4\" in device_name:\n",
    "    gpu_memory_utilization = 0.55\n",
    "else:\n",
    "    gpu_memory_utilization = 0.60\n",
    "\n",
    "print(\n",
    "    f\"Detected GPU: {device_name} ({total_vram_gib:.1f} GiB) — targeting {gpu_memory_utilization*100:.0f}% VRAM\",\n",
    " )\n",
    "\n",
    "set_torch_seed(config.seed)\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=config.base_model_id,\n",
    "    max_seq_length=config.max_seq_length,\n",
    "    dtype=None,  # auto-select\n",
    "    load_in_4bit=config.load_in_4bit,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=config.lora_rank,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    " )\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=config.lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=config.lora_rank * 2,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=config.seed,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    " )\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "print(\"Tokenizer configured (PAD token = EOS). Chat template:\", tokenizer.chat_template[:120], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5108edab",
   "metadata": {},
   "source": [
    "## 3) Dataset Preparation (Streaming)\n",
    "\n",
    "**Why:** Stream directly from Hugging Face so we stay within memory bounds while respecting the repository’s guidance against copying the entire 600k corpus into the editor.\n",
    "\n",
    "**Contract:** Every assistant turn must serialize back to the ground-truth schema; we normalize required keys before templating.\n",
    "\n",
    "**Safety:** Synthetic data only—no secrets or production telemetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d71c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming dataset helpers\n",
    "GROUND_TRUTH_DEFAULTS = {\n",
    "    \"version\": \"ground_truth_v1\",\n",
    "    \"enriched_findings\": [],\n",
    "    \"correlations\": [],\n",
    "    \"reductions\": {},\n",
    "    \"summaries\": {},\n",
    "    \"actions\": [],\n",
    "}\n",
    "\n",
    "\n",
    "def _clone_default(value):\n",
    "    if isinstance(value, dict):\n",
    "        return dict(value)\n",
    "    if isinstance(value, list):\n",
    "        return list(value)\n",
    "    return value\n",
    "\n",
    "\n",
    "def canonicalize_ground_truth(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    payload = record.get(\"ground_truth\") or record.get(\"data\") or record\n",
    "    canonical = dict(payload)\n",
    "    for key, default in GROUND_TRUTH_DEFAULTS.items():\n",
    "        current = canonical.get(key)\n",
    "        if current is None:\n",
    "            canonical[key] = _clone_default(default)\n",
    "        elif key == \"version\" and not isinstance(current, str):\n",
    "            canonical[key] = str(current)\n",
    "    return canonical\n",
    "\n",
    "\n",
    "def summarize_findings(payload: Dict[str, Any], limit: int) -> str:\n",
    "    rows = []\n",
    "    for finding in (payload.get(\"enriched_findings\") or [])[:limit]:\n",
    "        title = finding.get(\"title\", \"(untitled)\")\n",
    "        severity = finding.get(\"severity\", \"unknown\")\n",
    "        risk = finding.get(\"risk_score\", \"?\")\n",
    "        rows.append(f\"- [{severity}] {title} (risk_score={risk})\")\n",
    "    if not rows:\n",
    "        rows.append(\"- No enriched findings present in this slice.\")\n",
    "    return \"\\n\".join(rows)\n",
    "\n",
    "\n",
    "def summarize_correlations(payload: Dict[str, Any], limit: int) -> str:\n",
    "    rows = []\n",
    "    for corr in (payload.get(\"correlations\") or [])[:limit]:\n",
    "        title = corr.get(\"title\", \"(untitled)\")\n",
    "        related = \", \".join((corr.get(\"related_finding_ids\") or [])[:3]) or \"n/a\"\n",
    "        rows.append(f\"- {title} → related: {related}\")\n",
    "    if not rows:\n",
    "        rows.append(\"- No correlations linked in this slice.\")\n",
    "    return \"\\n\".join(rows)\n",
    "\n",
    "\n",
    "def build_messages(record: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "    payload = canonicalize_ground_truth(record)\n",
    "    summaries = payload.get(\"summaries\") or {}\n",
    "    exec_summary = summaries.get(\"executive_summary\") or \"No executive summary provided.\"\n",
    "    triage_summary = summaries.get(\"triage_summary\") or \"No triage summary provided.\"\n",
    "    user_prompt = textwrap.dedent(\n",
    "        f\"\"\"\n",
    "        Review the following synthetic security telemetry and produce a final ground truth JSON.\\n\\n\n",
    "        Top findings (capped at {config.max_findings_in_prompt}):\\n\n",
    "        {summarize_findings(payload, config.max_findings_in_prompt)}\\n\\n\n",
    "        Correlations (capped at {config.max_correlations_in_prompt}):\\n\n",
    "        {summarize_correlations(payload, config.max_correlations_in_prompt)}\\n\\n\n",
    "        Executive summary: {exec_summary}\\n\n",
    "        Triage summary: {triage_summary}\\n\\n\n",
    "        Respond with JSON that exactly matches the sys-scan ground_truth schema.\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "    assistant_json = json.dumps(payload, ensure_ascii=False, sort_keys=True)\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_json},\n",
    "    ]\n",
    "\n",
    "\n",
    "def to_sft_format(example: Dict[str, Any]) -> Dict[str, str]:\n",
    "    chat = build_messages(example)\n",
    "    rendered = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "    return {\"text\": rendered}\n",
    "\n",
    "\n",
    "def create_sft_dataset(tokenizer, *, max_samples: int | None = None, shuffle: bool = True):\n",
    "    stream = load_dataset(\n",
    "        config.dataset_repo_id,\n",
    "        split=config.dataset_split,\n",
    "        streaming=True,\n",
    "    )\n",
    "    if shuffle:\n",
    "        stream = stream.shuffle(seed=config.seed, buffer_size=config.shuffle_buffer)\n",
    "    if max_samples is not None:\n",
    "        stream = stream.take(max_samples)\n",
    "    return stream.map(to_sft_format)\n",
    "\n",
    "\n",
    "def create_eval_dataset(tokenizer, sample_count: int) -> Dataset:\n",
    "    eval_stream = load_dataset(\n",
    "        config.dataset_repo_id,\n",
    "        split=config.dataset_split,\n",
    "        streaming=True,\n",
    "    )\n",
    "    materialized = []\n",
    "    for raw in itertools.islice(eval_stream, sample_count):\n",
    "        materialized.append(to_sft_format(raw))\n",
    "    return Dataset.from_list(materialized) if materialized else Dataset.from_list([])\n",
    "\n",
    "\n",
    "train_dataset = create_sft_dataset(tokenizer)\n",
    "eval_dataset = create_eval_dataset(tokenizer, config.eval_sample_count)\n",
    "\n",
    "print(\"Train dataset: streaming iterable (length computed per epoch)\")\n",
    "print(f\"Eval dataset materialized: {len(eval_dataset)} examples\")\n",
    "\n",
    "try:\n",
    "    sample_preview = next(iter(load_dataset(config.dataset_repo_id, split=config.dataset_split, streaming=True).take(1)))\n",
    "    preview_text = to_sft_format(sample_preview)[\"text\"]\n",
    "    print(\"Preview prompt snippet:\\n\", preview_text[:500], \"...\")\n",
    "except Exception as exc:\n",
    "    print(\"Preview unavailable (likely because of offline mode):\", exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline sanity check using the bundled synthetic example\n",
    "example_path = (Path(\"../synthetic_data/synthetic_dataset_example.json\")).resolve()\n",
    "if example_path.exists():\n",
    "    with open(example_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        local_blob = json.load(fh)\n",
    "    local_payload = local_blob.get(\"data\") or local_blob\n",
    "    rendered = to_sft_format(local_payload)[\"text\"]\n",
    "    print(\"Local example rendered characters:\", len(rendered))\n",
    "    print(\"Assistant JSON keys:\", list(canonicalize_ground_truth(local_payload).keys())[:10])\n",
    "else:\n",
    "    print(\"Local synthetic example not found; skipping offline sanity check.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf58db",
   "metadata": {},
   "source": [
    "## 4) Supervised Fine-Tuning (SFT)\n",
    "\n",
    "**Why:** Establish a strong JSON-grounded baseline before any preference optimization so downstream rewards stay well-behaved.\n",
    "\n",
    "**Bounds:** We train from a streaming dataset with small batches and explicit `max_steps` to avoid exhausting VRAM.\n",
    "\n",
    "**Telemetry:** Evaluation runs on a deterministic, materialized slice for consistent validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=str(config.sft_dir),\n",
    "    max_steps=config.max_train_steps,\n",
    "    per_device_train_batch_size=config.per_device_batch_size,\n",
    "    gradient_accumulation_steps=config.grad_accum_steps,\n",
    "    optim=\"adamw_8bit\",\n",
    "    save_steps=200,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    "    seed=config.seed,\n",
    "    remove_unused_columns=False,\n",
    " )\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=sft_args,\n",
    " )\n",
    "\n",
    "sft_metrics = sft_trainer.train()\n",
    "print(\"SFT training metrics:\", sft_metrics)\n",
    "\n",
    "sft_trainer.save_model(str(config.sft_dir))\n",
    "tokenizer.save_pretrained(str(config.sft_dir))\n",
    "print(f\"SFT artifacts saved to {config.sft_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d10cb",
   "metadata": {},
   "source": [
    "## 5) GRPO Training Configuration (Optional)\n",
    "\n",
    "**Why:** Once the SFT adapter is stable we can encourage structured reasoning with reward shaping.\n",
    "\n",
    "**Bounds:** Streaming prompts keep VRAM under control; feel free to skip this section if resources are tight.\n",
    "\n",
    "**Prerequisite:** Confirm the SFT checkpoint finished successfully before starting GRPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5802934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the SFT adapter as the starting point for GRPO\n",
    "grpo_model, grpo_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=str(config.sft_dir),\n",
    "    max_seq_length=config.max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=config.load_in_4bit,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=config.lora_rank,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    " )\n",
    "\n",
    "if grpo_tokenizer.pad_token is None:\n",
    "    grpo_tokenizer.pad_token = grpo_tokenizer.eos_token\n",
    "\n",
    "grpo_train_dataset = create_sft_dataset(grpo_tokenizer)\n",
    "print(\"GRPO dataset ready (streaming iterable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6993cc",
   "metadata": {},
   "source": [
    "## 6) GRPO Training Execution\n",
    "\n",
    "**Why:** Reinforce JSON validity and schema compliance using lightweight reward functions.\n",
    "\n",
    "**Safety:** Rewards focus on structure only; no offensive content is generated or rewarded.\n",
    "\n",
    "**Resource Guard:** We check VRAM reservations before starting generation-heavy steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d967932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "REQUIRED_TOP_LEVEL_KEYS = tuple(GROUND_TRUTH_DEFAULTS.keys())\n",
    "\n",
    "\n",
    "def _extract_json(completion: str) -> Dict[str, Any]:\n",
    "    start = completion.find(\"{\")\n",
    "    end = completion.rfind(\"}\")\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        raise ValueError(\"No JSON object detected\")\n",
    "    snippet = completion[start : end + 1]\n",
    "    return json.loads(snippet)\n",
    "\n",
    "\n",
    "def reward_valid_json(completion: str, **_: Any) -> float:\n",
    "    try:\n",
    "        _extract_json(completion)\n",
    "        return 1.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def reward_has_required_keys(completion: str, **_: Any) -> float:\n",
    "    try:\n",
    "        data = _extract_json(completion)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    missing = [key for key in REQUIRED_TOP_LEVEL_KEYS if key not in data]\n",
    "    return 1.0 if not missing else 0.2\n",
    "\n",
    "\n",
    "grpo_args = GRPOConfig(\n",
    "    output_dir=str(config.grpo_dir),\n",
    "    max_steps=config.grpo_max_steps,\n",
    "    per_device_train_batch_size=config.grpo_per_device_batch_size,\n",
    "    gradient_accumulation_steps=config.grpo_grad_accum_steps,\n",
    "    optim=\"adamw_8bit\",\n",
    "    save_steps=200,\n",
    "    logging_steps=10,\n",
    "    learning_rate=config.grpo_learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    seed=config.seed,\n",
    "    report_to=\"none\",\n",
    "    max_completion_length=1024,\n",
    "    num_generations=config.grpo_num_generations,\n",
    "    beta=config.grpo_beta,\n",
    " )\n",
    "\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=grpo_model,\n",
    "    processing_class=grpo_tokenizer,\n",
    "    reward_funcs=[reward_valid_json, reward_has_required_keys],\n",
    "    args=grpo_args,\n",
    "    train_dataset=grpo_train_dataset,\n",
    " )\n",
    "\n",
    "reserved_vram = torch.cuda.memory_reserved(0) / 1024**3\n",
    "if reserved_vram > total_vram_gib * 0.85:\n",
    "    raise RuntimeError(\n",
    "        f\"VRAM too high before GRPO ({reserved_vram:.2f} GiB of {total_vram_gib:.1f} GiB). Lower batch size or num_generations.\"\n",
    "    )\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "grpo_metrics = grpo_trainer.train()\n",
    "print(\"GRPO training metrics:\", grpo_metrics)\n",
    "\n",
    "grpo_trainer.save_model(str(config.grpo_dir))\n",
    "grpo_tokenizer.save_pretrained(str(config.grpo_dir))\n",
    "print(f\"GRPO artifacts saved to {config.grpo_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cff2dd",
   "metadata": {},
   "source": [
    "## 7) Evaluation and Inference\n",
    "\n",
    "**Why:** Sanity-check the latest adapter on a streamed hold-out example and confirm that the response parses as JSON.\n",
    "\n",
    "**Auditability:** We log both the rendered completion and the JSON validation result.\n",
    "\n",
    "**Tip:** If you skip GRPO, point `model_path` at `config.sft_dir` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be756a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from vllm import SamplingParams\n",
    "\n",
    "def build_generation_messages(record: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "    chat = build_messages(record)\n",
    "    return chat[:-1]  # drop assistant turn so the model must regenerate it\n",
    "\n",
    "candidate_paths = [config.grpo_dir, config.sft_dir]\n",
    "model_path = next((path for path in candidate_paths if Path(path).exists() and any(Path(path).iterdir())), None)\n",
    "if model_path is None:\n",
    "    raise FileNotFoundError(\"Neither GRPO nor SFT checkpoints were found. Train before running inference.\")\n",
    "\n",
    "eval_model, eval_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=str(model_path),\n",
    "    max_seq_length=config.max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=config.load_in_4bit,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank=config.lora_rank,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    " )\n",
    "FastLanguageModel.for_inference(eval_model)\n",
    "\n",
    "eval_source = load_dataset(config.dataset_repo_id, split=config.dataset_split, streaming=True)\n",
    "sample = next(iter(eval_source.take(1)))\n",
    "messages = build_generation_messages(sample)\n",
    "prompt_text = eval_tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    " )\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "outputs = eval_model.fast_generate(\n",
    "    prompt_text,\n",
    "    sampling_params=sampling_params,\n",
    "    lora_request=None,\n",
    ")\n",
    "\n",
    "completion = outputs[0].outputs[0].text.strip()\n",
    "print(\"Generated response:\\n\", completion)\n",
    "\n",
    "try:\n",
    "    parsed = _extract_json(completion)\n",
    "    print(\"✅ Parsed JSON with keys:\", list(parsed.keys())[:10])\n",
    "except Exception as exc:\n",
    "    print(\"⚠️ JSON validation failed:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8806fcc0",
   "metadata": {},
   "source": [
    "## 8) Artifact Export\n",
    "\n",
    "**Why:** Persist LoRA weights for deployment (and optional conversion to GGUF for lightweight inference).\n",
    "\n",
    "**Verification:** Assert tensors are non-zero before pushing anywhere public.\n",
    "\n",
    "**Next step:** Use Unsloth Dynamic to package adapters into GGUF if you plan to mirror `unsloth/Qwen3-VL-2B-Thinking-GGUF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "model_for_export = globals().get(\"eval_model\") or globals().get(\"grpo_model\") or model\n",
    "export_base = Path(config.grpo_dir if Path(config.grpo_dir).exists() else config.sft_dir)\n",
    "lora_dir = export_base / \"lora_adapter\"\n",
    "lora_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_for_export.save_lora(str(lora_dir))\n",
    "\n",
    "with safe_open(lora_dir / \"adapter_model.safetensors\", framework=\"pt\") as handle:\n",
    "    for tensor_name in handle.keys():\n",
    "        tensor = handle.get_tensor(tensor_name)\n",
    "        if torch.count_nonzero(tensor) == 0:\n",
    "            raise ValueError(f\"Tensor {tensor_name} appears to be all zeros\")\n",
    "\n",
    "print(f\"GRPO/SFT LoRA adapters saved and verified in {lora_dir}\")\n",
    "print(\"Ready for optional GGUF export with Unsloth Dynamic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13e49d",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- **Streaming first:** Training pulls samples on-demand, so we never materialize the 600k-record corpus locally.\n",
    "- **Determinism:** Seeds propagate through Python, CUDA, and HF Trainer to keep runs reproducible.\n",
    "- **Deployment:** For quantized inference, start from `config.quantized_repo_hint` after exporting LoRA weights.\n",
    "- **Safety:** The assistant is confined to defensive reasoning with schema compliance checks baked into rewards.\n",
    "- **Next:** Integrate the exported adapters into the DAG or convert to GGUF via Unsloth Dynamic tooling."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
