{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen3 Security Agent: SFT \u2192 GRPO Pipeline\n",
        "**Full ML Pipeline on L4 GPU**\n",
        "\n",
        "This notebook implements a two-stage training pipeline:\n",
        "1.  **Stage 1: Supervised Fine-Tuning (SFT)**: Teaches the model the strict JSON schema and formatting required for the security analysis task using `SFTConfig`.\n",
        "2.  **Stage 2: GRPO (Reinforcement Learning)**: Optimizes the SFT model's reasoning using `GRPOConfig` to improve Risk Score accuracy and Severity assessment.\n",
        "\n",
        "**Configuration:**\n",
        "* **Source Install**: Bleeding-edge `unsloth`, `trl`, and `transformers`.\n",
        "* **Modern Configs**: Uses `SFTConfig` and `GRPOConfig`.\n",
        "* **L4 Optimization**: `bfloat16`, 24GB VRAM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths\n",
        "BASE_PATH = \"/content/drive/MyDrive/Qwen3_Security_Agent_Pipeline\"\n",
        "CACHE_DIR = os.path.join(BASE_PATH, \"cache\")\n",
        "SFT_OUTPUT_DIR = os.path.join(BASE_PATH, \"sft_checkpoints\")\n",
        "GRPO_OUTPUT_DIR = os.path.join(BASE_PATH, \"grpo_checkpoints\")\n",
        "\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(SFT_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(GRPO_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"SFT Checkpoints: {SFT_OUTPUT_DIR}\")\n",
        "print(f\"GRPO Checkpoints: {GRPO_OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "\n",
        "# 1. Clean Environment (Fix Pillow Conflicts)\n",
        "!pip uninstall -y pillow\n",
        "!pip install \"pillow<11.0\"\n",
        "\n",
        "# 2. Source Installs for Bleeding Edge Features\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/trl.git\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install git+https://github.com/huggingface/accelerate.git\n",
        "\n",
        "# 3. Latest Wheels for Compiled Libs\n",
        "!pip install --upgrade --no-cache-dir vllm bitsandbytes xformers\n",
        "\n",
        "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "# System Prompt (Shared across pipeline)\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert security analysis agent.\n",
        "1. Analyze the provided system scan finding.\n",
        "2. Reason step-by-step about impact, exploitability, and remediation inside <think> tags.\n",
        "3. Output the final structured analysis as a valid JSON object inside <answer> tags.\n",
        "\n",
        "The JSON output must strictly follow this schema:\n",
        "{\n",
        "  \"risk_score\": int,      // 0-100\n",
        "  \"severity\": \"string\",   // low, medium, high, critical\n",
        "  \"rationale\": \"string\"   // Brief summary of the risk justification\n",
        "}\"\"\"\n",
        "\n",
        "print(\"Streaming dataset...\")\n",
        "# We load a larger buffer to split between SFT and GRPO\n",
        "dataset = load_dataset(\n",
        "    \"jmazz/sys-scan-linux-synthetic\",\n",
        "    data_files=\"findings/batch_*.jsonl.gz\", \n",
        "    split=\"train\",\n",
        "    streaming=True,\n",
        "    cache_dir=CACHE_DIR\n",
        ")\n",
        "\n",
        "# Materialize 2000 samples\n",
        "raw_data = list(dataset.take(2000))\n",
        "\n",
        "# Split: 500 for SFT (Format Alignment), 1500 for GRPO (Reasoning Optimization)\n",
        "sft_data_raw = raw_data[:500]\n",
        "grpo_data_raw = raw_data[500:]\n",
        "\n",
        "print(f\"SFT Samples: {len(sft_data_raw)}\")\n",
        "print(f\"GRPO Samples: {len(grpo_data_raw)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1: Supervised Fine-Tuning (SFT)\n",
        "We first train the model to adhere to the strict JSON output format using standard Supervised Fine-Tuning. \n",
        "This ensures the model knows *how* to speak before we teach it *what* to think.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "\n",
        "# 1. Load Base Model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-4B-Instruct\",\n",
        "    max_seq_length = 4096,\n",
        "    load_in_4bit = True,\n",
        "    max_lora_rank = 64,\n",
        "    gpu_memory_utilization = 0.8,\n",
        "    cache_dir = CACHE_DIR,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 64,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "# 2. Format Function for SFT (Standard Chat)\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = []\n",
        "    texts = []\n",
        "    for i in range(len(examples[\"title\"])):\n",
        "        input_payload = {\n",
        "            \"title\": examples[\"title\"][i],\n",
        "            \"description\": examples[\"description\"][i],\n",
        "            \"metadata\": examples[\"metadata\"][i],\n",
        "            \"category\": examples[\"category\"][i]\n",
        "        }\n",
        "        ground_truth = {\n",
        "            \"risk_score\": int(examples[\"risk_score\"][i]),\n",
        "            \"severity\": examples[\"severity\"][i],\n",
        "            \"rationale\": examples[\"rationale\"][i]\n",
        "        }\n",
        "\n",
        "        # Unsloth handles the chat template application efficiently\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": json.dumps(input_payload, indent=2)},\n",
        "            {\"role\": \"assistant\", \"content\": json.dumps(ground_truth)}\n",
        "        ]\n",
        "        texts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False))\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# 3. Convert Raw List to Dataset object for SFTTrainer\n",
        "from datasets import Dataset\n",
        "sft_dataset = Dataset.from_list(sft_data_raw)\n",
        "\n",
        "# 4. SFT Configuration (Using Modern SFTConfig)\n",
        "sft_config = SFTConfig(\n",
        "    output_dir = SFT_OUTPUT_DIR,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 4096,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can be True for speed, but False is safer for complex JSON\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    warmup_steps = 10,\n",
        "    max_steps = 100, # Quick format alignment\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),\n",
        "    bf16 = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps = 1,\n",
        "    report_to = \"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = sft_dataset,\n",
        "    formatting_func = formatting_prompts_func,\n",
        "    args = sft_config,\n",
        "    processing_class = tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Starting Phase 1: SFT Training...\")\n",
        "trainer.train()\n",
        "\n",
        "# 5. Save SFT Adapters\n",
        "model.save_lora(os.path.join(SFT_OUTPUT_DIR, \"final_sft_adapter\"))\n",
        "print(\"SFT Adapter Saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: GRPO Training\n",
        "We now reload the model with the SFT adapters applied and train it using Group Relative Policy Optimization.\n",
        "This phase uses the **Continuous Risk Score** reward to refine the model's judgment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Reload Model to Ensure Clean State (Optional but Recommended)\n",
        "# For Unsloth, we can just continue, but reloading ensures we aren't carrying over optimizer states weirdly.\n",
        "del model, trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load Base Again\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-4B-Instruct\",\n",
        "    max_seq_length = 4096,\n",
        "    load_in_4bit = True,\n",
        "    max_lora_rank = 64,\n",
        "    gpu_memory_utilization = 0.8,\n",
        "    cache_dir = CACHE_DIR,\n",
        ")\n",
        "\n",
        "# Load the SFT Adapter we just trained\n",
        "model.load_lora(os.path.join(SFT_OUTPUT_DIR, \"final_sft_adapter\"))\n",
        "print(\"Loaded SFT Adapter for GRPO Phase.\")\n",
        "\n",
        "# 2. Format Data for GRPO (Prompt + Answer separate)\n",
        "def format_for_grpo(sample):\n",
        "    input_payload = {\n",
        "        \"title\": sample.get(\"title\"),\n",
        "        \"description\": sample.get(\"description\"),\n",
        "        \"metadata\": sample.get(\"metadata\", {}),\n",
        "        \"category\": sample.get(\"category\", \"general\")\n",
        "    }\n",
        "    ground_truth = {\n",
        "        \"risk_score\": int(sample.get(\"risk_score\", 0)),\n",
        "        \"severity\": sample.get(\"severity\", \"info\").lower(),\n",
        "        \"rationale\": sample.get(\"rationale\", \"\")\n",
        "    }\n",
        "    return {\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": json.dumps(input_payload, indent=2)}\n",
        "        ],\n",
        "        \"answer\": json.dumps(ground_truth)\n",
        "    }\n",
        "\n",
        "grpo_dataset = Dataset.from_list(grpo_data_raw).map(format_for_grpo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "# Reward Functions (Same as before)\n",
        "def json_format_reward(completions, **kwargs):\n",
        "    rewards = []\n",
        "    for completion in completions:\n",
        "        match = re.search(r\"<answer>(.*?)</answer>\", completion[0][\"content\"], re.DOTALL)\n",
        "        if not match:\n",
        "            rewards.append(-1.0)\n",
        "            continue\n",
        "        try:\n",
        "            json.loads(match.group(1))\n",
        "            rewards.append(1.0)\n",
        "        except:\n",
        "            rewards.append(-0.5)\n",
        "    return rewards\n",
        "\n",
        "def risk_score_accuracy_reward(completions, answer, **kwargs):\n",
        "    rewards = []\n",
        "    for completion, gt_str in zip(completions, answer):\n",
        "        try:\n",
        "            content = re.search(r\"<answer>(.*?)</answer>\", completion[0][\"content\"], re.DOTALL).group(1)\n",
        "            pred = float(json.loads(content).get(\"risk_score\", -1))\n",
        "            gt = float(json.loads(gt_str).get(\"risk_score\", -1))\n",
        "            if not (0 <= pred <= 100):\n",
        "                rewards.append(-1.0)\n",
        "            else:\n",
        "                diff = abs(pred - gt)\n",
        "                score = 1.0 - (diff / 100.0)\n",
        "                if diff <= 5: score += 0.5\n",
        "                rewards.append(score)\n",
        "        except:\n",
        "            rewards.append(0.0)\n",
        "    return rewards\n",
        "\n",
        "def severity_ordinal_reward(completions, answer, **kwargs):\n",
        "    ranks = {\"info\": 1, \"low\": 2, \"medium\": 3, \"high\": 4, \"critical\": 5}\n",
        "    rewards = []\n",
        "    for completion, gt_str in zip(completions, answer):\n",
        "        try:\n",
        "            content = re.search(r\"<answer>(.*?)</answer>\", completion[0][\"content\"], re.DOTALL).group(1)\n",
        "            pred = json.loads(content).get(\"severity\", \"\").lower()\n",
        "            gt = json.loads(gt_str).get(\"severity\", \"\").lower()\n",
        "            if pred == gt:\n",
        "                rewards.append(1.0)\n",
        "            else:\n",
        "                dist = abs(ranks.get(pred,0) - ranks.get(gt,0))\n",
        "                rewards.append(max(0.0, 1.0 - (dist * 0.25)))\n",
        "        except:\n",
        "            rewards.append(0.0)\n",
        "    return rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# GRPO Config (Using Modern Class)\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=GRPO_OUTPUT_DIR,\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_generations=4, \n",
        "    max_prompt_length=1024,\n",
        "    max_completion_length=1024,\n",
        "    max_steps=300, \n",
        "    save_steps=50,\n",
        "    report_to=\"none\",\n",
        "    use_vllm=True,\n",
        "    vllm_gpu_memory_utilization=0.5,\n",
        "    bf16=True, \n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[json_format_reward, risk_score_accuracy_reward, severity_ordinal_reward],\n",
        "    args=grpo_config,\n",
        "    train_dataset=grpo_dataset,\n",
        ")\n",
        "\n",
        "print(\"Starting Phase 2: GRPO Training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save Final Pipeline Model\n",
        "model.save_lora(os.path.join(GRPO_OUTPUT_DIR, \"final_pipeline_adapter\"))\n",
        "print(\"Pipeline Complete. Model Saved.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}