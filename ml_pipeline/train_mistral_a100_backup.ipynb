{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f7831",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt update && sudo apt install -y git python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b284bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/J-mazz/sys-scan-agent_MLops.git\n",
    "%cd sys-scan-agent_MLops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b892993",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ml_pipeline\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb06eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Optimizations for A100\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# A100 GPU optimizations\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Verify GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Create directory\n",
    "os.makedirs(\"massive_datasets\", exist_ok=True)\n",
    "\n",
    "# Download the tar.gz file from S3\n",
    "subprocess.run([\n",
    "    \"aws\", \"s3\", \"cp\", \n",
    "    \"s3://amazon-sagemaker-025547754238-us-east-1-4c58ffaa3dcd/dzd-bx6atbbltd2nvr/4l7iq4l5bmjkmf/ml_pipeline/massive_datasets.tar.gz\",\n",
    "    \"./\"\n",
    "], check=True)\n",
    "\n",
    "# Extract the tar.gz file into the massive_datasets directory\n",
    "subprocess.run([\"tar\", \"-xzf\", \"massive_datasets.tar.gz\", \"-C\", \"massive_datasets\", \"--strip-components=1\"], check=True)\n",
    "\n",
    "# Verify the data structure\n",
    "print(\"Data structure:\")\n",
    "for root, dirs, files in os.walk(\"massive_datasets\"):\n",
    "    level = root.replace(\"massive_datasets\", \"\").count(os.sep)\n",
    "    indent = \" \" * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = \" \" * 2 * (level + 1)\n",
    "    for file in files[:5]:  # Show first 5 files\n",
    "        print(f\"{subindent}{file}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"{subindent}... and {len(files)-5} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_from_disk\n",
    "import bitsandbytes  # For 8-bit Lion optimizer\n",
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_from_disk\n",
    "import bitsandbytes  # For 8-bit Lion optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c672b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gzip\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def preprocess_and_save_data() -> None:\n",
    "    \"\"\"\n",
    "    Finds, processes, and saves raw JSON findings to a persistent location.\n",
    "    \n",
    "    The function:\n",
    "    1. Reads JSON files from the input directory\n",
    "    2. Decompresses gzipped content if present\n",
    "    3. Extracts security findings\n",
    "    4. Formats them for training\n",
    "    5. Creates and saves a train/validation split dataset\n",
    "    \"\"\"\n",
    "    # Define paths - updated to use current directory\n",
    "    dataset_dir = os.path.abspath(\"./massive_datasets\")  # Changed from \"../massive_datasets\"\n",
    "    output_dir = os.path.abspath(\".\")\n",
    "    output_path = os.path.join(output_dir, \"processed_dataset\")\n",
    "\n",
    "    # Validate input directory\n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        raise FileNotFoundError(f\"Input directory not found at '{dataset_dir}'\")\n",
    "        \n",
    "    print(f\"Starting pre-processing from: {dataset_dir}\")\n",
    "\n",
    "    # Rest of the code remains the same...\n",
    "    file_paths = [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(dataset_dir)\n",
    "        for file in files\n",
    "        if file.startswith(\"batch_\") and file.endswith(\".json\")\n",
    "    ]\n",
    "\n",
    "    if not file_paths:\n",
    "        raise ValueError(f\"No matching JSON files found in {dataset_dir}\")\n",
    "\n",
    "    all_findings: List[Dict[str, Any]] = []\n",
    "    for file_path in tqdm(file_paths, desc=\"Processing raw files\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                content = json.load(infile)\n",
    "                if content.get('compressed'):\n",
    "                    try:\n",
    "                        hex_string = content['data']\n",
    "                        compressed_bytes = bytes.fromhex(hex_string)\n",
    "                        decompressed_json_string = gzip.decompress(compressed_bytes).decode('utf-8')\n",
    "                        decompressed_data = json.loads(decompressed_json_string)\n",
    "                        \n",
    "                        if 'data' in decompressed_data and 'findings' in decompressed_data['data']:\n",
    "                            findings_by_category = decompressed_data['data']['findings']\n",
    "                            for category, severity_levels in findings_by_category.items():\n",
    "                                if isinstance(severity_levels, dict):\n",
    "                                    for severity, findings_list in severity_levels.items():\n",
    "                                        if isinstance(findings_list, list):\n",
    "                                            all_findings.extend(findings_list)\n",
    "                    except (ValueError, KeyError) as e:\n",
    "                        print(f\"Error processing compressed data in {file_path}: {e}\")\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_findings:\n",
    "        raise ValueError(\"No findings were extracted from the input files\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Extracted {len(all_findings)} total findings.\")\n",
    "    print(\"\\nFormatting findings for training...\")\n",
    "    \n",
    "    formatted_records = [\n",
    "        {\n",
    "            \"text\": f\"Analyze the following security finding and provide an assessment:\\n\\n{json.dumps(record, indent=2)}\"\n",
    "        }\n",
    "        for record in tqdm(all_findings, desc=\"Formatting\")\n",
    "    ]\n",
    "\n",
    "    print(f\"Formatted {len(formatted_records)} findings for training.\")\n",
    "    print(\"\\nCreating, splitting, and saving the dataset...\")\n",
    "\n",
    "    full_dataset = Dataset.from_list(formatted_records)\n",
    "    train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    split_dataset = DatasetDict({\n",
    "        'train': train_test_split['train'],\n",
    "        'validation': train_test_split['test']\n",
    "    })\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    split_dataset.save_to_disk(output_path)\n",
    "\n",
    "    print(f\"\\n‚úÖ SUCCESS! Dataset saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        preprocess_and_save_data()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a33290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        # Model configuration\n",
    "        self.base_model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "        self.new_model_name = \"sys-scan-agent\"\n",
    "        \n",
    "        # Optimizer parameters\n",
    "        self.learning_rate = 1e-4\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.99\n",
    "        self.weight_decay = 0.01\n",
    "        \n",
    "        # Training parameters\n",
    "        self.epochs = 3\n",
    "        self.batch_size = 8\n",
    "\n",
    "# Create an instance of the config\n",
    "config = TrainingConfig()\n",
    "\n",
    "print(\"Training configuration initialized:\")\n",
    "print(f\"Base model: {config.base_model_name}\")\n",
    "print(f\"New model name: {config.new_model_name}\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Epochs: {config.epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fcd0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "print(f\"Loading tokenizer and model {config.base_model_name}...\")\n",
    "\n",
    "# Set Hugging Face token for authentication\n",
    "import os\n",
    "os.environ['HF_TOKEN'] = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "\n",
    "# Authenticate with Hugging Face (required for Mistral models)\n",
    "from huggingface_hub import login\n",
    "try:\n",
    "    hf_token = os.environ.get("HF_TOKEN")
    if hf_token:
        login(token=hf_token)\n",
    "    print(\"‚úÖ Hugging Face authentication successful\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Hugging Face login failed: {e}\")\n",
    "    print(\"Continuing anyway...\")\n",
    "\n",
    "# Import required libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# LoRA configuration for memory efficiency\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Rank\n",
    "    lora_alpha=32,          # Alpha scaling\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name,\n",
    "    device_map=\"auto\",  # Use accelerate for device mapping\n",
    "    torch_dtype=torch.float16,  # Use FP16 for A100 GPU\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully. Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from bitsandbytes.optim import Lion8bit\n",
    "\n",
    "# 2. Create Lion optimizer\n",
    "optimizer = Lion8bit(\n",
    "    params=model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=(config.beta_1, config.beta_2),\n",
    "    weight_decay=config.weight_decay,\n",
    "    min_8bit_size=4096,\n",
    "    percentile_clipping=100,\n",
    "    block_wise=True\n",
    ")\n",
    "\n",
    "# 3. Create training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./checkpoints/{config.new_model_name}\",\n",
    "    num_train_epochs=config.epochs,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=config.batch_size,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # Optimizer configuration\n",
    "    optim=\"lion_8bit\",\n",
    "    learning_rate=config.learning_rate,\n",
    "    optim_args=dict(\n",
    "        betas=(config.beta_1, config.beta_2),\n",
    "        weight_decay=config.weight_decay,\n",
    "        min_8bit_size=4096,\n",
    "        percentile_clipping=100,\n",
    "        block_wise=True\n",
    "    ),\n",
    "    \n",
    "    # Rest of the training arguments remain the same\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    logging_first_step=True,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    torch_compile=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Learning rate: {config.learning_rate}, Weight decay: {config.weight_decay}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee614ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "dataset_path = \"./processed_dataset\"\n",
    "print(f\"Loading pre-processed dataset from {dataset_path}...\")\n",
    "split_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "print(f\"Train samples: {len(split_dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(split_dataset['validation'])}\")\n",
    "\n",
    "# Optional: Display a few examples from the dataset\n",
    "print(\"\\nExample training samples:\")\n",
    "for i, example in enumerate(split_dataset['train'].select(range(2))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(example['text'][:500] + \"...\")  # Show first 500 chars of each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from bitsandbytes.optim import Lion8bit\n",
    "import torch\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Rank\n",
    "    lora_alpha=32,          # Alpha scaling\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Initialize SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['validation'],\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "# Override optimizer with Lion8bit\n",
    "optimizer = Lion8bit(\n",
    "    params=model.parameters(),\n",
    "    lr=config.learning_rate,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU memory before training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory before training: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB used\")\n",
    "    print(f\"GPU Memory cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\nüöÄ Starting fine-tuning with TRL SFTTrainer and Lion optimizer on A100 GPU...\")\n",
    "print(f\"Training for {config.epochs} epochs with batch size {config.batch_size}\")\n",
    "print(f\"Expected training time: ~{len(split_dataset['train']) * config.epochs / (config.batch_size * 2):.0f} steps\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU memory after training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory after training: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB used\")\n",
    "    print(f\"GPU Memory cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8befd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "print(\"‚úÖ Fine-tuning completed!\")\n",
    "\n",
    "print(f\"Saving model to {config.new_model_name}...\")\n",
    "trainer.save_model(config.new_model_name)\n",
    "tokenizer.save_pretrained(config.new_model_name)\n",
    "\n",
    "print(f\"üéâ Model and tokenizer saved to {config.new_model_name}\")\n",
    "\n",
    "# Print training summary\n",
    "training_stats = trainer.state.log_history\n",
    "if training_stats:\n",
    "    final_loss = training_stats[-1].get('train_loss', 'N/A')\n",
    "    print(f\"Final training loss: {final_loss}\")\n",
    "    print(f\"Total training steps: {trainer.state.global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f24393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model for embedded deployment\n",
    "print(\"üîÑ Starting extreme quantization for embedded deployment...\")\n",
    "\n",
    "# Install quantization dependencies if needed\n",
    "!pip install auto-gptq safetensors optimum --quiet\n",
    "\n",
    "# Run the quantization script\n",
    "!python quantize_models.py\n",
    "\n",
    "print(\"‚úÖ Quantization complete! Check models/deployment_package/ for the quantized model chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73642591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test inference with the trained model\n",
    "def test_inference():\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    print(\"\\nüß™ Testing inference with trained model...\")\n",
    "    \n",
    "    # Load the trained model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    trained_model = PeftModel.from_pretrained(base_model, config.new_model_name)\n",
    "    \n",
    "    # Test prompt\n",
    "    test_prompt = \"\"\"Analyze the following security finding and provide an assessment:\n",
    "\n",
    "{\n",
    "  \"type\": \"process\",\n",
    "  \"name\": \"suspicious_process\",\n",
    "  \"pid\": 1234,\n",
    "  \"risk_score\": 0.8,\n",
    "  \"command\": \"/bin/bash -c 'curl malicious-site.com'\"\n",
    "}\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Test Response:\")\n",
    "    print(response[len(test_prompt):].strip())\n",
    "\n",
    "# Uncomment to test inference\n",
    "# test_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fca4e2",
   "metadata": {},
   "source": [
    "## Training Complete! üéâ\n",
    "\n",
    "### Summary:\n",
    "- **Model**: Mistral-7B-Instruct fine-tuned with LoRA\n",
    "- **GPU**: A100 with optimized memory usage\n",
    "- **Optimizer**: Lion8bit (8-bit quantized for efficiency)\n",
    "- **Training**: Mixed precision FP16\n",
    "- **Dataset**: Security scan findings (2.5M examples)\n",
    "\n",
    "### Next Steps:\n",
    "1. **Quantization**: Model automatically quantized to <400MB total\n",
    "2. **Chunking**: Split into multiple <50MB safetensors files\n",
    "3. **Deployment**: Ready for embedded sys-scan-graph integration\n",
    "4. **Testing**: Validate inference with security scenarios\n",
    "\n",
    "### Output Locations:\n",
    "- **LoRA Adapters**: `sys-scan-agent/`\n",
    "- **Quantized Chunks**: `models/deployment_package/model_chunk_*.safetensors`\n",
    "- **Total Size**: <400MB across all chunks\n",
    "\n",
    "The model is now optimized for embedded deployment with extreme compression while maintaining security analysis capabilities!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6f423",
   "metadata": {},
   "source": [
    "# Mistral-7B Fine-tuning on A100 GPU\n",
    "\n",
    "This notebook fine-tunes the Mistral-7B-Instruct model on security scan data using LoRA and the Lion optimizer, optimized for A100 GPU performance.\n",
    "\n",
    "## Setup Overview:\n",
    "- **Model**: mistralai/Mistral-7B-Instruct-v0.1\n",
    "- **GPU**: A100 with 24GB VRAM\n",
    "- **Optimizer**: Lion (proven efficient for this dataset)\n",
    "- **Technique**: LoRA fine-tuning\n",
    "- **Precision**: Mixed FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a10c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e9ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Optimizations for A100\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# A100 GPU optimizations\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Verify GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7170acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load massive dataset from S3\n",
    "!mkdir -p massive_datasets\n",
    "aws s3 sync s3://amazon-sagemaker-025547754238-us-east-1-4c58ffaa3dcd/dzd-bx6atbbltd2nvr/4l7iq4l5bmjkmf/ml_pipeline/massive_datasets.tar.gz ./\n",
    "!tar -xzf massive_datasets.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_from_disk\n",
    "import bitsandbytes  # For 8-bit Lion optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e67d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import gzip\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_and_save_data():\n",
    "    \"\"\"\n",
    "    Finds, processes, and saves raw JSON findings to a persistent GDrive location.\n",
    "    \"\"\"\n",
    "    # --- Define Correct Absolute Paths ---\n",
    "    # Path to the directory containing your raw data files\n",
    "    dataset_dir = \"../massive_datasets\"\n",
    "    \n",
    "    # Path where the final processed dataset will be saved\n",
    "    output_dir = \".\"\n",
    "    output_path = os.path.join(output_dir, \"processed_dataset\")\n",
    "\n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        print(f\"‚ùå ERROR: Input directory not found at '{dataset_dir}'.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Starting pre-processing from: {dataset_dir}\")\n",
    "\n",
    "    # (The rest of the script remains the same)\n",
    "    file_paths = []\n",
    "    for root, _, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            if file.startswith(\"batch_\") and file.endswith(\".json\"):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "\n",
    "    all_findings = []\n",
    "    for file_path in tqdm(file_paths, desc=\"Processing raw files\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                content = json.load(infile)\n",
    "                if content.get('compressed'):\n",
    "                    hex_string = content['data']\n",
    "                    compressed_bytes = bytes.fromhex(hex_string)\n",
    "                    decompressed_json_string = gzip.decompress(compressed_bytes).decode('utf-8')\n",
    "                    decompressed_data = json.loads(decompressed_json_string)\n",
    "                    if 'data' in decompressed_data and 'findings' in decompressed_data['data']:\n",
    "                        findings_by_category = decompressed_data['data']['findings']\n",
    "                        for category, severity_levels in findings_by_category.items():\n",
    "                            if isinstance(severity_levels, dict):\n",
    "                                for severity, findings_list in severity_levels.items():\n",
    "                                    if isinstance(findings_list, list):\n",
    "                                        all_findings.extend(findings_list)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file {file_path} due to error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extracted {len(all_findings)} total findings.\")\n",
    "    print(\"\\nFormatting findings for training...\")\n",
    "    formatted_records = []\n",
    "    for record in tqdm(all_findings, desc=\"Formatting\"):\n",
    "        formatted_text = f\"Analyze the following security finding and provide an assessment:\\n\\n{json.dumps(record, indent=2)}\"\n",
    "        formatted_records.append({\"text\": formatted_text})\n",
    "\n",
    "    print(f\"Formatted {len(formatted_records)} findings for training.\")\n",
    "    print(\"\\nCreating, splitting, and saving the dataset...\")\n",
    "    if not formatted_records:\n",
    "        print(\"‚ùå No records found to create a dataset. Exiting.\")\n",
    "        return\n",
    "\n",
    "    full_dataset = Dataset.from_list(formatted_records)\n",
    "    train_test_split = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    split_dataset = DatasetDict({'train': train_test_split['train'], 'validation': train_test_split['test']})\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    split_dataset.save_to_disk(output_path)\n",
    "\n",
    "    print(f\"\\n‚úÖ SUCCESS! Dataset saved to persistent location: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_and_save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        self.epochs = 3\n",
    "        self.learning_rate = 2e-4\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.99\n",
    "        self.weight_decay = 0.01\n",
    "        self.batch_size = 8\n",
    "        self.model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "        self.new_model_name = \"sys-scan-mistral-agent\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55492559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "print(f\"Loading tokenizer and model {config.model_name}...\")\n",
    "\n",
    "# Set Hugging Face token for authentication\n",
    "import os\n",
    "    \"# NOTE: Set your token via: export HF_TOKEN='your_token_here'\\n\",\n",
    "    \"hf_token = os.environ.get('HF_TOKEN', 'your_token_here')\\n\",\n",
    "\n",
    "# Authenticate with Hugging Face (required for Mistral models)\n",
    "from huggingface_hub import login\n",
    "try:\n",
    "    hf_token = os.environ.get("HF_TOKEN")
    if hf_token:
        login(token=hf_token)\n",
    "    print(\"‚úÖ Hugging Face authentication successful\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Hugging Face login failed: {e}\")\n",
    "    print(\"Continuing anyway...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# LoRA configuration for memory efficiency\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Good balance for A100\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\",  # Use accelerate for device mapping\n",
    "    torch_dtype=torch.float16,  # Use FP16 for A100 GPU\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully. Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc4486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lion optimizer (bitsandbytes version for better memory efficiency)\n",
    "from bitsandbytes.optim import Lion8bit\n",
    "\n",
    "optimizer = Lion8bit(\n",
    "    params=model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=(config.beta_1, config.beta_2),\n",
    "    weight_decay=config.weight_decay,\n",
    "    min_8bit_size=4096,  # Minimum tensor size for 8-bit optimization\n",
    "    percentile_clipping=100,  # Adaptive gradient clipping\n",
    "    block_wise=True  # Independent quantization per block\n",
    ")\n",
    "\n",
    "print(\"Lion8bit optimizer configured with 8-bit quantization for memory efficiency\")\n",
    "print(f\"Learning rate: {config.learning_rate}, Weight decay: {config.weight_decay}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments optimized for A100 GPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./checkpoints/{config.new_model_name}\",\n",
    "    num_train_epochs=config.epochs,\n",
    "    per_device_train_batch_size=config.batch_size,        # 8 per device for A100\n",
    "    per_device_eval_batch_size=config.batch_size,\n",
    "    gradient_accumulation_steps=2,        # Effective batch size: 16\n",
    "    optim=\"adamw_torch\",  # Will override with Lion\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,                           # Mixed precision for A100\n",
    "    bf16=False,                          # Use FP16 instead of BF16\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,          # Enable for GPU\n",
    "    gradient_checkpointing=True,         # Memory optimization\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured for A100 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_path = \"./processed_dataset\"\n",
    "print(f\"Loading pre-processed dataset from {dataset_path}...\")\n",
    "split_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "print(f\"Train samples: {len(split_dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(split_dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7077052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['validation'],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "# Override optimizer with Lion\n",
    "trainer.optimizer = optimizer\n",
    "\n",
    "print(\"SFT Trainer initialized with LoRA and Lion optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace31b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU memory before training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory before training: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB used\")\n",
    "    print(f\"GPU Memory cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e3298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\nüöÄ Starting fine-tuning with TRL SFTTrainer and Lion optimizer on A100 GPU...\")\n",
    "print(f\"Training for {config.epochs} epochs with batch size {config.batch_size}\")\n",
    "print(f\"Expected training time: ~{len(split_dataset['train']) * config.epochs / (config.batch_size * 2):.0f} steps\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164171cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU memory after training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory after training: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB used\")\n",
    "    print(f\"GPU Memory cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "print(\"‚úÖ Fine-tuning completed!\")\n",
    "\n",
    "print(f\"Saving model to {config.new_model_name}...\")\n",
    "trainer.save_model(config.new_model_name)\n",
    "tokenizer.save_pretrained(config.new_model_name)\n",
    "\n",
    "print(f\"üéâ Model and tokenizer saved to {config.new_model_name}\")\n",
    "\n",
    "# Print training summary\n",
    "training_stats = trainer.state.log_history\n",
    "if training_stats:\n",
    "    final_loss = training_stats[-1].get('train_loss', 'N/A')\n",
    "    print(f\"Final training loss: {final_loss}\")\n",
    "    print(f\"Total training steps: {trainer.state.global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model for embedded deployment\n",
    "print(\"üîÑ Starting extreme quantization for embedded deployment...\")\n",
    "\n",
    "# Install quantization dependencies if needed\n",
    "!pip install auto-gptq safetensors optimum --quiet\n",
    "\n",
    "# Run the quantization script\n",
    "!python quantize_models.py\n",
    "\n",
    "print(\"‚úÖ Quantization complete! Check models/deployment_package/ for the quantized model chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f412bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test inference with the trained model\n",
    "def test_inference():\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    print(\"\\nüß™ Testing inference with trained model...\")\n",
    "    \n",
    "    # Load the trained model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    trained_model = PeftModel.from_pretrained(base_model, config.new_model_name)\n",
    "    \n",
    "    # Test prompt\n",
    "    test_prompt = \"\"\"Analyze the following security finding and provide an assessment:\n",
    "\n",
    "{\n",
    "  \"type\": \"process\",\n",
    "  \"name\": \"suspicious_process\",\n",
    "  \"pid\": 1234,\n",
    "  \"risk_score\": 0.8,\n",
    "  \"command\": \"/bin/bash -c 'curl malicious-site.com'\"\n",
    "}\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Test Response:\")\n",
    "    print(response[len(test_prompt):].strip())\n",
    "\n",
    "# Uncomment to test inference\n",
    "# test_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d626389",
   "metadata": {},
   "source": [
    "## Training Complete! üéâ\n",
    "\n",
    "### Summary:\n",
    "- **Model**: Mistral-7B-Instruct fine-tuned with LoRA\n",
    "- **GPU**: A100 with optimized memory usage\n",
    "- **Optimizer**: Lion8bit (8-bit quantized for efficiency)\n",
    "- **Training**: Mixed precision FP16\n",
    "- **Dataset**: Security scan findings\n",
    "\n",
    "### Next Steps:\n",
    "1. **Quantization**: Model automatically quantized to <400MB total\n",
    "2. **Chunking**: Split into multiple <50MB safetensors files\n",
    "3. **Deployment**: Ready for embedded sys-scan-graph integration\n",
    "4. **Testing**: Validate inference with security scenarios\n",
    "\n",
    "### Output Locations:\n",
    "- **LoRA Adapters**: `sys-scan-mistral-agent-a100-lora/`\n",
    "- **Quantized Chunks**: `models/deployment_package/model_chunk_*.safetensors`\n",
    "- **Total Size**: <400MB across all chunks\n",
    "\n",
    "The model is now optimized for embedded deployment with extreme compression while maintaining security analysis capabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb996463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from botocore.config import Config\n",
    "\n",
    "# AWS S3 Configuration\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    region_name='us-east-1',\n",
    "    config=Config(\n",
    "        retries={'max_attempts': 3, 'mode': 'standard'},\n",
    "        use_dualstack_endpoint=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Download massive dataset from S3\n",
    "bucket_name = 'sys-scan-agent-data'\n",
    "dataset_key = 'massive_datasets.tar.gz'\n",
    "local_dataset_path = '/tmp/massive_datasets.tar.gz'\n",
    "\n",
    "print(\"Downloading massive dataset from S3...\")\n",
    "try:\n",
    "    s3_client.download_file(bucket_name, dataset_key, local_dataset_path)\n",
    "    print(f\"Downloaded {dataset_key} to {local_dataset_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    print(\"Make sure AWS credentials are configured and bucket exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a23775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def preprocess_and_save_data(dataset_path: str, output_dir: str = \"./processed_data\") -> Dataset:\n",
    "    \"\"\"\n",
    "    Preprocess the massive security scan dataset for training.\n",
    "    Extracts 2.5M security findings from compressed JSON batches.\n",
    "    \"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "\n",
    "    # Create output directory\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # Extract tar.gz file\n",
    "    print(f\"Extracting {dataset_path}...\")\n",
    "    with tarfile.open(dataset_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=output_dir)\n",
    "\n",
    "    # Find all batch files\n",
    "    batch_files = list(Path(output_dir).glob(\"massive_datasets_max/batch_*.json\"))\n",
    "    print(f\"Found {len(batch_files)} batch files\")\n",
    "\n",
    "    all_findings = []\n",
    "    total_findings = 0\n",
    "\n",
    "    # Process each batch file\n",
    "    for batch_file in sorted(batch_files):\n",
    "        print(f\"Processing {batch_file.name}...\")\n",
    "\n",
    "        try:\n",
    "            with gzip.open(batch_file, 'rt', encoding='utf-8') as f:\n",
    "                batch_data = json.load(f)\n",
    "\n",
    "            # Extract enriched_findings from each scan\n",
    "            for scan in batch_data.get('scans', []):\n",
    "                findings = scan.get('enriched_findings', [])\n",
    "                all_findings.extend(findings)\n",
    "                total_findings += len(findings)\n",
    "\n",
    "                # Progress update every 100k findings\n",
    "                if total_findings % 100000 == 0:\n",
    "                    print(f\"Processed {total_findings} findings...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {batch_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Total findings extracted: {total_findings}\")\n",
    "\n",
    "    # Create training examples\n",
    "    training_examples = []\n",
    "    for finding in all_findings:\n",
    "        # Format as instruction-response pairs for security analysis\n",
    "        instruction = f\"Analyze this security finding: {finding.get('description', 'Unknown finding')}\"\n",
    "        response = f\"Finding details: {json.dumps(finding, indent=2)}\"\n",
    "\n",
    "        training_examples.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"output\": response\n",
    "        })\n",
    "\n",
    "    # Create HuggingFace dataset\n",
    "    dataset = Dataset.from_list(training_examples)\n",
    "\n",
    "    # Split into train/validation\n",
    "    train_test_split = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "    train_dataset = train_test_split['train']\n",
    "    val_dataset = train_test_split['test']\n",
    "\n",
    "    print(f\"Training examples: {len(train_dataset)}\")\n",
    "    print(f\"Validation examples: {len(val_dataset)}\")\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Process the downloaded dataset\n",
    "if os.path.exists(local_dataset_path):\n",
    "    train_dataset, val_dataset = preprocess_and_save_data(local_dataset_path)\n",
    "    print(\"Data preprocessing complete!\")\n",
    "else:\n",
    "    print(f\"Dataset not found at {local_dataset_path}\")\n",
    "    print(\"Please ensure the S3 download completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from bitsandbytes.optim import Lion8bit\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for the successful 7-hour training on 2.5M examples\"\"\"\n",
    "\n",
    "    # Model configuration\n",
    "    model_name: str = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "    max_seq_length: int = 512\n",
    "\n",
    "    # LoRA configuration (exact settings from successful run)\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    "\n",
    "    # Lion optimizer configuration (exact parameters from successful run)\n",
    "    lion_lr: float = 1e-4\n",
    "    lion_betas: tuple = (0.9, 0.99)\n",
    "    lion_weight_decay: float = 0.01\n",
    "\n",
    "    # Training arguments (optimized for A100 and 2.5M examples)\n",
    "    per_device_train_batch_size: int = 4\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    num_train_epochs: int = 1\n",
    "    learning_rate: float = 1e-4  # Will be overridden by Lion optimizer\n",
    "    warmup_steps: int = 100\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 500\n",
    "    save_total_limit: int = 3\n",
    "\n",
    "    # Mixed precision and memory optimization\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False  # A100 supports TF32, so we use FP16\n",
    "    gradient_checkpointing: bool = True\n",
    "    dataloader_num_workers: int = 4\n",
    "    dataloader_pin_memory: bool = True\n",
    "\n",
    "    # Output configuration\n",
    "    output_dir: str = \"./mistral-security-finetuned\"\n",
    "    logging_dir: str = \"./logs\"\n",
    "\n",
    "# Initialize configuration\n",
    "config = TrainingConfig()\n",
    "print(\"Training configuration initialized:\")\n",
    "print(f\"- Model: {config.model_name}\")\n",
    "print(f\"- LoRA rank: {config.lora_r}\")\n",
    "print(f\"- Lion learning rate: {config.lion_lr}\")\n",
    "print(f\"- Batch size: {config.per_device_train_batch_size}\")\n",
    "print(f\"- Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "print(f\"- Max sequence length: {config.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d163d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Configure 4-bit quantization for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading Mistral-7B-Instruct model and tokenizer...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "# Set pad token to eos token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"- Model type: {type(model)}\")\n",
    "print(f\"- Device: {model.device}\")\n",
    "print(f\"- Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    target_modules=config.lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(f\"LoRA applied! Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Verify model is properly configured\n",
    "print(\"\\\\nModel configuration verified:\")\n",
    "print(f\"- LoRA rank: {lora_config.r}\")\n",
    "print(f\"- LoRA alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"- Target modules: {lora_config.target_modules}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
