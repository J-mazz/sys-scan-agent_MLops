{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020dd02e",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 3 with Distributed Training on AWS\n",
    "\n",
    "This notebook fine-tunes the Llama 3 8B model using distributed training with TensorFlow's MirroredStrategy on a g5.4xlarge instance with 2 A10 GPUs.\n",
    "\n",
    "## Features:\n",
    "- Distributed training across multiple GPUs using MirroredStrategy\n",
    "- Lion optimizer for memory efficiency\n",
    "- Dynamic sequence padding\n",
    "- Configurable hyperparameters\n",
    "- Checkpointing and early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572d3e3",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "Before running this notebook on AWS:\n",
    "\n",
    "1. **Launch g5.4xlarge instance** with Deep Learning AMI\n",
    "2. **Upload this notebook** to the instance\n",
    "3. **Upload `massive_dataset.tar.gz`** to the instance (from your local massive_datasets_max folder)\n",
    "4. **Run the setup cells** below to clone the repo and extract data\n",
    "5. **Install dependencies**: `pip install transformers datasets keras tensorflow`\n",
    "\n",
    "## AWS Instance Specs\n",
    "- **Instance**: g5.4xlarge\n",
    "- **GPUs**: 2x NVIDIA A10G (24GB each)\n",
    "- **vCPUs**: 16\n",
    "- **RAM**: 128GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/Mazzlabs/sys-scan-agent_MLops.git\n",
    "%cd sys-scan-agent_MLops/ml_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload and extract the dataset\n",
    "# Note: Manually upload massive_dataset.tar.gz to the instance first\n",
    "!tar -xzf ../massive_dataset.tar.gz -C ./\n",
    "!ls -la massive_datasets_max/ | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install transformers datasets keras tensorflow ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83446af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras import layers, ops\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Set Keras backend to TensorFlow\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocesses text data with dynamic padding.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(dataset, tokenizer, batch_size):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow dataset from a Hugging Face dataset.\n",
    "    \"\"\"\n",
    "    def generator():\n",
    "        for example in dataset:\n",
    "            processed = preprocess_function({\"text\": example[\"text\"]}, tokenizer)\n",
    "            yield {\n",
    "                \"input_ids\": processed[\"input_ids\"],\n",
    "                \"attention_mask\": processed[\"attention_mask\"],\n",
    "                \"labels\": processed[\"labels\"]\n",
    "            }\n",
    "\n",
    "    output_signature = {\n",
    "        \"input_ids\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "        \"attention_mask\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "        \"labels\": tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    }\n",
    "\n",
    "    tf_dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "\n",
    "    tf_dataset = tf_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe59a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Widgets\n",
    "epochs_widget = widgets.IntSlider(value=1, min=1, max=10, description='Epochs:')\n",
    "learning_rate_widget = widgets.FloatLogSlider(value=2e-4, min=-6, max=-2, step=0.1, description='Learning Rate:')\n",
    "beta_1_widget = widgets.FloatSlider(value=0.9, min=0.0, max=1.0, step=0.01, description='Beta 1:')\n",
    "beta_2_widget = widgets.FloatSlider(value=0.99, min=0.0, max=1.0, step=0.01, description='Beta 2:')\n",
    "weight_decay_widget = widgets.FloatSlider(value=0.01, min=0.0, max=0.1, step=0.001, description='Weight Decay:')\n",
    "\n",
    "display(epochs_widget, learning_rate_widget, beta_1_widget, beta_2_widget, weight_decay_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9500ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_keras():\n",
    "    \"\"\"\n",
    "    Fine-tunes the Llama 3 model using TensorFlow's MirroredStrategy for multi-GPU training.\n",
    "    \"\"\"\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"âœ… Found {strategy.num_replicas_in_sync} GPUs. Using MirroredStrategy.\")\n",
    "\n",
    "    # Get values from widgets\n",
    "    epochs = epochs_widget.value\n",
    "    learning_rate = learning_rate_widget.value\n",
    "    beta_1 = beta_1_widget.value\n",
    "    beta_2 = beta_2_widget.value\n",
    "    weight_decay = weight_decay_widget.value\n",
    "\n",
    "    dataset_path = \"./processed_dataset\"\n",
    "    print(f\"Loading pre-processed dataset from {dataset_path}...\")\n",
    "    split_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "    new_model_name = \"sys-scan-llama-agent-keras3-lion\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    batch_size_per_replica = 8\n",
    "    global_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n",
    "\n",
    "    print(f\"Batch size per GPU: {batch_size_per_replica}\")\n",
    "    print(f\"Global batch size: {global_batch_size}\")\n",
    "\n",
    "    train_dataset = create_tf_dataset(split_dataset['train'], tokenizer, global_batch_size)\n",
    "    val_dataset = create_tf_dataset(split_dataset['validation'], tokenizer, global_batch_size)\n",
    "\n",
    "    with strategy.scope():\n",
    "        print(f\"Loading model {model_name} for Keras 3...\")\n",
    "        model = TFAutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        print(\"Creating Lion optimizer with configured parameters...\")\n",
    "        optimizer = keras.optimizers.Lion(\n",
    "            learning_rate=learning_rate,\n",
    "            beta_1=beta_1,\n",
    "            beta_2=beta_2,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    "        )\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f\"./checkpoints/{new_model_name}_epoch_{{epoch:02d}}\",\n",
    "            save_freq='epoch',\n",
    "            save_weights_only=True\n",
    "        ),\n",
    "        keras.callbacks.TensorBoard(log_dir=\"./logs\"),\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "    ]\n",
    "\n",
    "    os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "    os.makedirs(\"./logs\", exist_ok=True)\n",
    "\n",
    "    print(\"\\nðŸš€ Starting fine-tuning with Keras, Lion, and MirroredStrategy...\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"âœ… Fine-tuning completed!\")\n",
    "\n",
    "    print(f\"Saving model to {new_model_name}...\")\n",
    "    model.save_pretrained(new_model_name)\n",
    "    tokenizer.save_pretrained(new_model_name)\n",
    "\n",
    "    print(f\"ðŸŽ‰ Model and tokenizer saved to {new_model_name}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "history = train_with_keras()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
