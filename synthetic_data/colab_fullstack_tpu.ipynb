{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8a04a7",
   "metadata": {},
   "source": [
    "# Sys-Scan Full-Stack Nightly TPU Fine-Tuning\n",
    "\n",
    "This notebook executes the end-to-end TRL supervised fine-tuning pipeline on a Google Colab TPU v3/v4 instance using the full-stack nightly configuration described in `finetuneguide.instructions.md`.\n",
    "\n",
    "- Installs synchronized nightly builds of `torch_xla`, `optimum-tpu`, and core Hugging Face libraries\n",
    "- Formats the massive synthetic security dataset into Mistral-style instruction/response prompts\n",
    "- Enables TPU FSDP v2 with LoRA adapters and the Lion optimizer\n",
    "- Provides validation and troubleshooting guidance for common TPU runtime issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c6537",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "1. Configure Colab for TPU execution and install the nightly PyTorch/XLA stack.\n",
    "2. Mount Google Drive, extract the massive dataset archive, and hydrate the JSON files.\n",
    "3. Transform each ground-truth record into the supervision prompt template expected by `SFTTrainer`.\n",
    "4. Load `mistralai/Mistral-7B-Instruct`, apply LoRA adapters, and enable FSDP v2 via `optimum.tpu`.\n",
    "5. Launch supervised fine-tuning with Lion, monitor metrics, and export the LoRA adapter for Sys-Scan-Graph integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76c2b2",
   "metadata": {},
   "source": [
    "## Step 0 – Switch the runtime to TPU\n",
    "\n",
    "Before running any code cells, open **Runtime → Change runtime type** in Colab and choose **TPU** as the hardware accelerator. Then reconnect the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa9b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nightly torch/torchvision/torch_xla installation. Expect a runtime restart afterwards.\n",
    "!pip install --quiet numpy torch torchvision torch_xla[tpu] -f https://storage.googleapis.com/libtpu-wheels/index.html -f https://storage.googleapis.com/libtpu-releases/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9baae0e",
   "metadata": {},
   "source": [
    "## Step 1 – Confirm TPU availability after the runtime restart\n",
    "\n",
    "After the previous cell finishes, Colab may automatically restart the runtime. Rerun the cells below to configure the PJRT environment and validate TPU visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a36d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.runtime as xr\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n",
    "os.environ.setdefault(\"LIBTPU_INIT_TPU_ON_DEVICE\", \"1\")\n",
    "print(f'PJRT_DEVICE set to {os.environ[\"PJRT_DEVICE\"]}')\n",
    "\n",
    "try:\n",
    "    init_fn = getattr(xr, \"initialize_system\", None)\n",
    "    if init_fn is None:\n",
    "        init_fn = getattr(xr, \"initialize_tpu_system\", None)\n",
    "    if callable(init_fn):\n",
    "        init_fn()\n",
    "    else:\n",
    "        # Fallback: touch the default device to trigger PJRT startup\n",
    "        xm.xla_device()\n",
    "\n",
    "    devices = []\n",
    "    if hasattr(torch_xla, \"real_devices\") and callable(torch_xla.real_devices):\n",
    "        devices = torch_xla.real_devices()\n",
    "    if not devices:\n",
    "        devices = xm.get_xla_supported_devices()\n",
    "    device_strings = [str(d) for d in devices]\n",
    "    print(\"XLA devices:\", device_strings)\n",
    "    assert any(str(d).startswith(\"TPU\") for d in device_strings), \"TPU not detected.\"\n",
    "    print(f\"Successfully connected to {len(device_strings)} TPU cores.\")\n",
    "except Exception as exc:\n",
    "    print(\"XLA initialization failed. Please restart the runtime and rerun the installation cell.\")\n",
    "    raise exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install synchronized nightly builds of the Hugging Face ecosystem required for the TRL TPU pipeline.\n",
    "!pip install --quiet git+https://github.com/huggingface/transformers.git\n",
    "!pip install --quiet git+https://github.com/huggingface/datasets.git\n",
    "!pip install --quiet git+https://github.com/huggingface/accelerate.git\n",
    "!pip install --quiet git+https://github.com/huggingface/peft.git\n",
    "!pip install --quiet git+https://github.com/huggingface/trl.git\n",
    "!pip install --quiet --no-deps git+https://github.com/huggingface/optimum-tpu.git  # --no-deps avoids downgrading nightly torch_xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f5db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import accelerate\n",
    "import peft\n",
    "import trl\n",
    "import importlib.metadata\n",
    "\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"peft:\", peft.__version__)\n",
    "print(\"trl:\", trl.__version__)\n",
    "print(\"optimum-tpu:\", importlib.metadata.version(\"optimum-tpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7e63f",
   "metadata": {},
   "source": [
    "## Step 2 – Mount Google Drive for dataset access\n",
    "\n",
    "This workflow reads `massive_datasets.tar.gz` from Drive. Mount your Google Drive before extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d39a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "tar_path = \"/content/drive/MyDrive/sys-scan-graph/massive_datasets/massive_datasets.tar.gz\"\n",
    "extract_path = Path(\"/content/massive_datasets\")\n",
    "\n",
    "extract_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_path)\n",
    "\n",
    "RAW_SYN_DIR = extract_path\n",
    "children = [entry.name for entry in extract_path.iterdir()]\n",
    "print(f\"Extracted data to {extract_path}\")\n",
    "print(f\"Top-level entries: {children}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ea0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "import gzip\n",
    "from datasets import Dataset\n",
    "\n",
    "RAW_SYN_DIR = Path(RAW_SYN_DIR)\n",
    "\n",
    "def collect_shards(root: Path):\n",
    "    patterns = [(\"jsonl\", \"*.jsonl\"), (\"json.gz\", \"*.json.gz\"), (\"json\", \"*.json\")]\n",
    "    for label, pattern in patterns:\n",
    "        matches = sorted(root.rglob(pattern))\n",
    "        if not matches:\n",
    "            continue\n",
    "\n",
    "        filtered = []\n",
    "        ignored = []\n",
    "        for path in matches:\n",
    "            name = path.name.lower()\n",
    "            if name.startswith(\"batch_\"):\n",
    "                filtered.append(path)\n",
    "            else:\n",
    "                ignored.append(path)\n",
    "\n",
    "        if filtered:\n",
    "            if ignored:\n",
    "                preview = [p.name for p in ignored[:5]]\n",
    "                if len(ignored) > 5:\n",
    "                    preview.append(\"...\")\n",
    "                print(f\"Ignoring {len(ignored)} non-shard file(s): {preview}\")\n",
    "            return label, filtered\n",
    "\n",
    "        # Matches exist but all were ignored; try next pattern\n",
    "    return None, []\n",
    "\n",
    "shard_type, shard_paths = collect_shards(RAW_SYN_DIR)\n",
    "\n",
    "print(f\"Scanning extracted data under {RAW_SYN_DIR}...\")\n",
    "if not shard_paths:\n",
    "    sample_dirs = sorted([p for p in RAW_SYN_DIR.glob(\"*\") if p.is_dir()])[:10]\n",
    "    sample_files = sorted([p for p in RAW_SYN_DIR.glob(\"*.*\") if p.is_file()])[:10]\n",
    "    raise FileNotFoundError(\n",
    "        \"No dataset shards detected. Inspect the archive layout.\\n\"\n",
    "        f\"Sample directories: {[p.as_posix() for p in sample_dirs]}\\n\"\n",
    "        f\"Sample files: {[p.as_posix() for p in sample_files]}\"\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(shard_paths)} {shard_type} shard(s). Example: {[p.as_posix() for p in shard_paths[:3]]}\")\n",
    "\n",
    "MAX_FINDINGS_PER_PROMPT = 24\n",
    "\n",
    "def sanitize_finding(finding: dict) -> dict:\n",
    "    cleaned = finding.copy()\n",
    "    cleaned.pop(\"_processed_at\", None)\n",
    "    cleaned.pop(\"_data_quality\", None)\n",
    "    risk_score = cleaned.get(\"risk_score\")\n",
    "    if isinstance(risk_score, float):\n",
    "        cleaned[\"risk_score\"] = round(risk_score)\n",
    "    probability = cleaned.get(\"probability_actionable\")\n",
    "    if isinstance(probability, float):\n",
    "        cleaned[\"probability_actionable\"] = round(probability, 3)\n",
    "    return cleaned\n",
    "\n",
    "def flatten_findings(findings_by_category: dict) -> list:\n",
    "    flattened = []\n",
    "    for severity_map in findings_by_category.values():\n",
    "        for entries in severity_map.values():\n",
    "            for entry in entries:\n",
    "                flattened.append(sanitize_finding(entry))\n",
    "    return flattened\n",
    "\n",
    "def sanitize_correlation(correlation: dict) -> dict:\n",
    "    cleaned = correlation.copy()\n",
    "    cleaned.pop(\"_processed_at\", None)\n",
    "    cleaned.pop(\"_correlation_strength\", None)\n",
    "    risk_score = cleaned.get(\"risk_score\")\n",
    "    if isinstance(risk_score, float):\n",
    "        cleaned[\"risk_score\"] = round(risk_score)\n",
    "    return cleaned\n",
    "\n",
    "SEVERITY_ORDER = {\"critical\": 4, \"high\": 3, \"medium\": 2, \"low\": 1, \"info\": 0}\n",
    "\n",
    "def build_summaries(correlation: dict, related_findings: list, stats: dict, verification: dict) -> dict:\n",
    "    severity_counts = Counter(f.get(\"severity\", \"unknown\") for f in related_findings)\n",
    "    category_counts = Counter(f.get(\"category\", \"unknown\") for f in related_findings)\n",
    "    top_categories = \", \".join(cat for cat, _ in category_counts.most_common(3))\n",
    "    exec_summary = (\n",
    "        f\"{correlation.get('title', 'Correlation')} ({correlation.get('severity', 'unknown')} severity, risk {correlation.get('risk_score', 'n/a')}) \"\n",
    "        f\"involves {len(related_findings)} finding(s) spanning {top_categories or 'multiple categories'}.\"\n",
    "    )\n",
    "    triage_ids = \", \".join(f.get(\"id\", \"?\") for f in related_findings[:10])\n",
    "    metrics = {\n",
    "        \"related_finding_count\": len(related_findings),\n",
    "        \"correlation_strength\": correlation.get(\"correlation_strength\"),\n",
    "        \"quality_score\": verification.get(\"quality_score\"),\n",
    "        \"severity_distribution\": dict(severity_counts)\n",
    "    }\n",
    "    return {\n",
    "        \"executive_summary\": exec_summary,\n",
    "        \"analyst\": correlation.get(\"description\", \"\"),\n",
    "        \"triage_summary\": triage_ids and f\"Prioritise remediation for finding IDs: {triage_ids}\" or \"Related finding identifiers unavailable.\",\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "\n",
    "def build_actions(correlation: dict, related_findings: list) -> list:\n",
    "    sorted_findings = sorted(\n",
    "        related_findings,\n",
    "        key=lambda f: (SEVERITY_ORDER.get(f.get(\"severity\", \"\"), 0), f.get(\"risk_score\", 0)),\n",
    "        reverse=True\n",
    "    )\n",
    "    actions = []\n",
    "    for idx, finding in enumerate(sorted_findings[:3], start=1):\n",
    "        severity = finding.get(\"severity\", \"medium\")\n",
    "        priority = \"high\" if severity in {\"critical\", \"high\"} else \"medium\"\n",
    "        description = finding.get(\"description\", \"\")\n",
    "        actions.append({\n",
    "            \"id\": f\"{correlation.get('id', 'corr')}_action_{idx}\",\n",
    "            \"title\": f\"Investigate {finding.get('title', 'finding')}\",\n",
    "            \"description\": (\n",
    "                f\"Validate finding {finding.get('id', 'unknown')} ({finding.get('category', 'unknown')}, severity {severity}) \"\n",
    "                f\"as part of correlation '{correlation.get('title', 'Correlation')}'. Summary: {description[:240]}\"\n",
    "            ),\n",
    "            \"priority\": priority,\n",
    "            \"severity\": severity\n",
    "        })\n",
    "    if not actions:\n",
    "        actions.append({\n",
    "            \"id\": f\"{correlation.get('id', 'corr')}_action_1\",\n",
    "            \"title\": f\"Review correlation {correlation.get('id', 'corr')}\",\n",
    "            \"description\": \"Correlation references no additional findings. Perform manual validation.\",\n",
    "            \"priority\": \"medium\",\n",
    "            \"severity\": correlation.get(\"severity\", \"medium\")\n",
    "        })\n",
    "    return actions\n",
    "\n",
    "def prompt_generator(paths):\n",
    "    for shard_path in paths:\n",
    "        shard_path = Path(shard_path)\n",
    "        try:\n",
    "            with shard_path.open() as handle:\n",
    "                raw_doc = json.load(handle)\n",
    "        except Exception as exc:\n",
    "            print(f\"⚠️ Could not read {shard_path.name}: {exc}\")\n",
    "            continue\n",
    "\n",
    "        records = raw_doc if isinstance(raw_doc, list) else [raw_doc]\n",
    "\n",
    "        for record in records:\n",
    "            hex_payload = record.get(\"data\")\n",
    "            if not hex_payload:\n",
    "                continue\n",
    "            try:\n",
    "                decoded = json.loads(gzip.decompress(bytes.fromhex(hex_payload)))\n",
    "            except Exception as exc:\n",
    "                print(f\"⚠️ Failed to decode {shard_path.name}: {exc}\")\n",
    "                continue\n",
    "\n",
    "            data = decoded.get(\"data\", {})\n",
    "            metadata = decoded.get(\"metadata\", {})\n",
    "            verification = metadata.get(\"verification_summary\", {})\n",
    "            stats = data.get(\"statistics\", {})\n",
    "\n",
    "            flattened_findings = flatten_findings(data.get(\"findings\", {}))\n",
    "            finding_lookup = {finding.get(\"id\"): finding for finding in flattened_findings if finding.get(\"id\")}\n",
    "\n",
    "            for correlation in data.get(\"correlations\", []):\n",
    "                sanitized_corr = sanitize_correlation(correlation)\n",
    "                related_ids = sanitized_corr.get(\"correlation_refs\") or []\n",
    "                related = [finding_lookup[fid] for fid in related_ids if fid in finding_lookup]\n",
    "                if not related:\n",
    "                    continue\n",
    "\n",
    "                if len(related) > MAX_FINDINGS_PER_PROMPT:\n",
    "                    related = sorted(related, key=lambda f: f.get(\"risk_score\", 0), reverse=True)[:MAX_FINDINGS_PER_PROMPT]\n",
    "\n",
    "                finding_payload = {\n",
    "                    \"version\": \"ground_truth_v1\",\n",
    "                    \"enriched_findings\": related,\n",
    "                    \"correlations\": [sanitized_corr],\n",
    "                    \"reductions\": {\n",
    "                        \"severity_distribution\": stats.get(\"severity_distribution\", {}),\n",
    "                        \"category_distribution\": stats.get(\"category_distribution\", {}),\n",
    "                        \"risk_score_stats\": stats.get(\"risk_score_stats\", {})\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                analysis_payload = {\n",
    "                    \"summaries\": build_summaries(sanitized_corr, related, stats, verification),\n",
    "                    \"actions\": build_actions(sanitized_corr, related)\n",
    "                }\n",
    "\n",
    "                prompt_text = (\n",
    "                    \"### Instruction:\\n\"\n",
    "                    \"Analyze the following security correlation and provide an assessment:\\n\"\n",
    "                    f\"{json.dumps(finding_payload, ensure_ascii=False)}\\n\\n\"\n",
    "                    \"### Response:\\n\"\n",
    "                    f\"{json.dumps(analysis_payload, ensure_ascii=False)}\"\n",
    "                )\n",
    "\n",
    "                yield {\"prompt\": prompt_text}\n",
    "\n",
    "correlation_prompt_dataset = Dataset.from_generator(lambda: prompt_generator(shard_paths))\n",
    "\n",
    "if len(correlation_prompt_dataset) == 0:\n",
    "    raise ValueError(\"Decoded zero prompts from the supplied shards.\")\n",
    "\n",
    "print(correlation_prompt_dataset)\n",
    "preview_prompt = correlation_prompt_dataset[:1][\"prompt\"][0]\n",
    "print(preview_prompt[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c0d20",
   "metadata": {},
   "source": [
    "## Step 3 – Convert correlations into SFT-friendly prompts\n",
    "\n",
    "Each shard now contains a normalized dataset with thousands of correlations. The previous cell decodes the hex-encoded gzip payloads, flattens the related findings, and assembles correlation windows into instruction/response pairs for supervised fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90bf97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SEED = 42\n",
    "EVAL_SPLIT = 0.001\n",
    "SAMPLE_SIZE = 512  # Further reduced to 512 for extreme stability on TPU\n",
    "\n",
    "shuffled_dataset = correlation_prompt_dataset.shuffle(seed=DATASET_SEED)\n",
    "if len(shuffled_dataset) > 1:\n",
    "    formatted_dataset = shuffled_dataset.train_test_split(test_size=EVAL_SPLIT, seed=DATASET_SEED)\n",
    "    train_dataset = formatted_dataset[\"train\"]\n",
    "    eval_dataset = formatted_dataset[\"test\"]\n",
    "else:\n",
    "    train_dataset = shuffled_dataset\n",
    "    eval_dataset = shuffled_dataset\n",
    "\n",
    "if SAMPLE_SIZE:\n",
    "    train_dataset = train_dataset.select(range(min(len(train_dataset), SAMPLE_SIZE)))\n",
    "    eval_cap = min(len(eval_dataset), max(1, SAMPLE_SIZE // 100))\n",
    "    eval_dataset = eval_dataset.select(range(eval_cap))\n",
    "\n",
    "print(f\"Train prompts: {len(train_dataset):,}\")\n",
    "print(f\"Eval prompts: {len(eval_dataset):,}\")\n",
    "\n",
    "sample_prompt = train_dataset[:1][\"prompt\"][0]\n",
    "print(sample_prompt[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cfeafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity checks on the generated prompt corpus\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "prompt_lengths = [len(p.split()) for p in train_dataset[:1024][\"prompt\"]]\n",
    "print(f\"Prompt word length — min: {min(prompt_lengths)}, median: {sorted(prompt_lengths)[len(prompt_lengths)//2]}, max: {max(prompt_lengths)}\")\n",
    "\n",
    "def extract_json_sections(prompt_text: str):\n",
    "    # Capture only the JSON part after \"assessment:\\n\"\n",
    "    match_instruction = re.search(r\"assessment:\\n(.+?)\\n\\n### Response:\\n\", prompt_text, flags=re.S)\n",
    "    match_response = re.search(r\"### Response:\\n(.+)$\", prompt_text, flags=re.S)\n",
    "    instruction_str = match_instruction.group(1) if match_instruction else \"\"\n",
    "    response_str = match_response.group(1) if match_response else \"\"\n",
    "    print(f\"Instruction match: {bool(match_instruction)}, length: {len(instruction_str)}\")\n",
    "    print(f\"Response match: {bool(match_response)}, length: {len(response_str)}\")\n",
    "    if instruction_str:\n",
    "        try:\n",
    "            instruction_payload = json.loads(instruction_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Instruction JSON error: {e}\")\n",
    "            print(f\"Instruction string preview: {instruction_str[:200]}...\")\n",
    "            instruction_payload = {}\n",
    "    else:\n",
    "        instruction_payload = {}\n",
    "    if response_str:\n",
    "        try:\n",
    "            response_payload = json.loads(response_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Response JSON error: {e}\")\n",
    "            print(f\"Response string preview: {response_str[:200]}...\")\n",
    "            response_payload = {}\n",
    "    else:\n",
    "        response_payload = {}\n",
    "    return instruction_payload, response_payload\n",
    "\n",
    "sample_instruction, sample_response = extract_json_sections(train_dataset[:1][\"prompt\"][0])\n",
    "print(\"Instruction keys:\", sample_instruction.keys())\n",
    "print(\"Response keys:\", sample_response.keys())\n",
    "print(\"Sample actions:\", sample_response.get(\"actions\", [])[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c5e25",
   "metadata": {},
   "source": [
    "## Step 4 – Configure model, LoRA adapters, and optimizer\n",
    "\n",
    "The configuration below fine-tunes `mistralai/Mistral-7B-Instruct-v0.1` with LoRA rank 16 adapters across attention and MLP layers, using Lion as the optimizer and enabling FSDP v2 for TPU sharding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ccaf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "OUTPUT_DIR = \"/content/sys-scan-mistral-lora\"\n",
    "MAX_SEQ_LENGTH = 256  # Reduced from 512 to ease memory pressure on TPU\n",
    "NUM_EPOCHS = 3\n",
    "PER_DEVICE_BATCH_SIZE = 1  # Reduced to 1 for maximum stability\n",
    "GRADIENT_ACCUMULATION_STEPS = 16  # Increased to compensate for smaller batch size\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOGGING_STEPS = 10\n",
    "SAVE_TOTAL_LIMIT = 2\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a900482",
   "metadata": {},
   "source": [
    "## Important: PEFT Application and Packing Requirements\n",
    "\n",
    "**Key API Change in TRL nightly:**\n",
    "- The `SFTTrainer` does **not** accept `peft_config` as a direct parameter\n",
    "- Instead, apply PEFT to the model **before** creating the trainer using `get_peft_model(model, peft_config)`\n",
    "- This is demonstrated in the next cell\n",
    "\n",
    "**About the Packing Warning:**\n",
    "When `packing=True` (enabled by default for efficiency), TRL will warn if the model's attention implementation is not set to a supported flash-attention variant. This warning appears during trainer initialization but **does not prevent training**. \n",
    "\n",
    "On TPU with PJRT, the default attention implementation typically works fine. However, if you encounter issues during training, you can:\n",
    "1. Disable packing: Set `packing=False` in SFTConfig\n",
    "2. Or set `attn_implementation` in the model config to `\"flash_attention_2\"` (if available on your TPU runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a8907",
   "metadata": {},
   "source": [
    "### Before creating the trainer\n",
    "- Run the dataset preparation cells in Step 3 to define `train_dataset` and `eval_dataset`.\n",
    "- If you see a NameError about missing datasets, re-run Step 3 then re-run the model and trainer cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b9ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "    bf16=True,\n",
    "    optim=\"lion_32bit\",  # Use Lion optimizer on TPU (8-bit variants not supported)\n",
    "    max_grad_norm=1.0,\n",
    "    eval_strategy=\"no\",  # Disable eval to reduce memory usage and avoid crashes\n",
    "    report_to=[\"none\"],  # Set to [\"tensorboard\"] if you want TB logs installed\n",
    "    dataloader_drop_last=True,\n",
    "    remove_unused_columns=False,\n",
    "    seed=SEED,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,  # Disabled to prevent kernel crashes during tokenization; re-enable if stable\n",
    "    **fsdp_training_args,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c5305",
   "metadata": {},
   "source": [
    "> Note: To revert to AdamW, change `optim=\"lion_32bit\"` to `optim=\"adamw_torch\"` in the SFTConfig above. TPU does not support bitsandbytes, so 8-bit variants like `lion_8bit` are not available. Eval is disabled (`eval_strategy=\"no\"`) to reduce memory usage. Packing is disabled to prevent crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56692db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "train_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3cf11",
   "metadata": {},
   "source": [
    "## Step 5 – Launch Training\n",
    "\n",
    "The trainer is now ready. If you see warnings about packing and flash-attention, these are informational and should not block training. The actual training launch happens in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11776ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_DIR = f\"{OUTPUT_DIR}/final_adapter\"\n",
    "trainer.save_model(ADAPTER_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"LoRA adapter saved to {ADAPTER_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bcbf9e",
   "metadata": {},
   "source": [
    "## Step 5 – Export and integrate the LoRA adapter\n",
    "\n",
    "- Download `/content/sys-scan-mistral-lora` (or move it to Drive) to persist checkpoints.\n",
    "- Copy the adapter directory into the Sys-Scan-Graph deployment and update its configuration to point at the new weights.\n",
    "- Keep the `trainer_state.json` and logs for future resumption or audit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982fd75",
   "metadata": {},
   "source": [
    "## Troubleshooting & Known TPU pitfalls\n",
    "\n",
    "- **XLA initialization errors** (`Failed to get global TPU topology`): Restart the runtime, rerun the torch_xla installation cell, and re-execute the validation cell.\n",
    "- **Missing `torch.xla` attributes**: Ensure every Hugging Face library was installed from its `main` branch via the commands above; mixing release builds with nightly torch_xla will fail.\n",
    "- **Runtime crash at end of epoch**: Confirm `dataloader_drop_last=True` remains set in `TrainingArguments` so that each TPU step receives a full batch.\n",
    "- **Out-of-memory or compile stalls**: Reduce `PER_DEVICE_BATCH_SIZE`, enable `SAMPLE_SIZE` for quick iterations, or lower `MAX_SEQ_LENGTH` for debugging runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b45614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "device = xm.xla_device()\n",
    "trainer.model.eval()\n",
    "sample_prompt = random.choice(eval_dataset[:5][\"prompt\"])\n",
    "inputs = tokenizer(sample_prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    generated_ids = trainer.model.generate(**inputs, max_new_tokens=256)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
