{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sys-Scan Security Model Fine-Tuning\n",
    "\n",
    "This notebook orchestrates the fine-tuning of a Mistral-7B model for security analysis tasks.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **Environment Setup**: Install dependencies and configure compute\n",
    "2. **Data Generation**: Generate or load synthetic security findings\n",
    "3. **Model Setup**: Load teacher and student models with quantization\n",
    "4. **Training**: Fine-tune with Lion optimizer and optional distillation\n",
    "5. **Evaluation**: Test on held-out security scenarios\n",
    "6. **Export**: Save model for deployment in sys-scan-graph\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- GPU with 24GB+ VRAM (A100, V100, or equivalent)\n",
    "- Python 3.10+\n",
    "- CUDA 11.8+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers==4.36.0 datasets==2.15.0 accelerate==0.25.0\n",
    "!pip install -q peft==0.7.0 bitsandbytes==0.41.3 trl==0.7.9\n",
    "!pip install -q sentencepiece protobuf tensorboard\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️  No GPU detected - training will be very slow!\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Models\n",
    "    \"teacher_model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"student_model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \n",
    "    # Data paths (adjust for your environment)\n",
    "    \"data_dir\": \"/home/joseph-mazzini/sys-scan-embedded-agent/synthetic_data\",\n",
    "    \"output_dir\": \"./mistral-7b-security-finetuned\",\n",
    "    \"cache_dir\": \"./cache\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"max_seq_length\": 2048,\n",
    "    \n",
    "    # LoRA\n",
    "    \"lora_r\": 64,\n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    # Distillation\n",
    "    \"use_distillation\": True,\n",
    "    \"temperature\": 2.0,\n",
    "    \"distillation_alpha\": 0.5,\n",
    "    \n",
    "    # Optimizer\n",
    "    \"use_lion\": True,\n",
    "    \"lion_beta1\": 0.9,\n",
    "    \"lion_beta2\": 0.99,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \n",
    "    # Quantization\n",
    "    \"use_4bit\": True,\n",
    "    \"use_bf16\": True,\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"cache_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "with open(f\"{CONFIG['output_dir']}/training_config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(\"✓ Configuration saved\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Generate new synthetic data\n",
    "# Uncomment and modify producer counts as needed\n",
    "\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append(CONFIG[\"data_dir\"])\n",
    "\n",
    "from synthetic_data_pipeline import run_synthetic_data_pipeline\n",
    "\n",
    "result = run_synthetic_data_pipeline(\n",
    "    output_path=f\"{CONFIG['data_dir']}/training_dataset.json\",\n",
    "    producer_counts={\n",
    "        \"processes\": 100,\n",
    "        \"network\": 80,\n",
    "        \"kernel_params\": 50,\n",
    "        \"modules\": 40,\n",
    "        \"world_writable\": 30,\n",
    "        \"suid\": 25,\n",
    "        \"ioc\": 60,\n",
    "        \"mac\": 20,\n",
    "        \"dns\": 70,\n",
    "        \"endpoint_behavior\": 50\n",
    "    },\n",
    "    use_langchain=False,  # Set True if you have LangChain API key\n",
    "    compress=False,\n",
    "    conservative_parallel=False  # Use True for local dev, False for cloud\n",
    ")\n",
    "\n",
    "print(f\"✓ Generated {result['findings_count']} findings\")\n",
    "\"\"\"\n",
    "\n",
    "# Option B: Use existing data\n",
    "data_files = list(Path(CONFIG[\"data_dir\"]).glob(\"*.json\"))\n",
    "data_files = [str(f) for f in data_files if \"schema\" not in str(f) and \"config\" not in str(f)]\n",
    "\n",
    "print(f\"Found {len(data_files)} data files:\")\n",
    "for f in data_files:\n",
    "    print(f\"  - {Path(f).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one dataset to inspect\n",
    "with open(data_files[0]) as f:\n",
    "    sample_data = json.load(f)\n",
    "\n",
    "print(\"Dataset structure:\")\n",
    "print(json.dumps({k: type(v).__name__ for k, v in sample_data.items()}, indent=2))\n",
    "\n",
    "# Check findings\n",
    "if 'data' in sample_data:\n",
    "    findings = sample_data['data'].get('findings', {})\n",
    "else:\n",
    "    findings = sample_data.get('findings', {})\n",
    "\n",
    "print(f\"\\nFinding categories: {list(findings.keys())}\")\n",
    "\n",
    "# Count total findings\n",
    "total_findings = 0\n",
    "for category, severity_groups in findings.items():\n",
    "    if isinstance(severity_groups, dict):\n",
    "        for findings_list in severity_groups.values():\n",
    "            if isinstance(findings_list, list):\n",
    "                total_findings += len(findings_list)\n",
    "\n",
    "print(f\"Total findings: {total_findings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Fine-Tuning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline modules\n",
    "import sys\n",
    "sys.path.append(CONFIG[\"data_dir\"])\n",
    "\n",
    "from ml_finetuning_pipeline import (\n",
    "    FinetuningConfig,\n",
    "    SecurityFineTuningPipeline,\n",
    "    SecurityDataPreprocessor\n",
    ")\n",
    "from lion_optimizer import Lion, create_lion_optimizer\n",
    "from distillation_trainer import create_distillation_trainer, DistillationTrainer\n",
    "\n",
    "# Create pipeline config from notebook config\n",
    "pipeline_config = FinetuningConfig(\n",
    "    teacher_model_name=CONFIG[\"teacher_model\"],\n",
    "    student_model_name=CONFIG[\"student_model\"],\n",
    "    lora_r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    use_4bit=CONFIG[\"use_4bit\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    use_lion=CONFIG[\"use_lion\"],\n",
    "    lion_beta1=CONFIG[\"lion_beta1\"],\n",
    "    lion_beta2=CONFIG[\"lion_beta2\"],\n",
    "    use_distillation=CONFIG[\"use_distillation\"],\n",
    "    distillation_temperature=CONFIG[\"temperature\"],\n",
    "    distillation_alpha=CONFIG[\"distillation_alpha\"],\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    cache_dir=CONFIG[\"cache_dir\"],\n",
    "    bf16=CONFIG[\"use_bf16\"],\n",
    ")\n",
    "\n",
    "print(\"✓ Pipeline configuration created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = SecurityFineTuningPipeline(pipeline_config)\n",
    "\n",
    "# Setup tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = pipeline.setup_tokenizer()\n",
    "print(f\"✓ Tokenizer loaded (vocab size: {len(tokenizer)})\")\n",
    "\n",
    "# Setup models\n",
    "print(\"\\nLoading models (this may take several minutes)...\")\n",
    "teacher_model, student_model = pipeline.setup_models()\n",
    "print(\"✓ Models loaded\")\n",
    "\n",
    "# Print trainable parameters\n",
    "if hasattr(student_model, 'print_trainable_parameters'):\n",
    "    student_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset from synthetic data files\n",
    "print(f\"Preparing dataset from {len(data_files)} files...\")\n",
    "\n",
    "dataset = pipeline.prepare_dataset(\n",
    "    data_paths=data_files,\n",
    "    train_split=0.9\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dataset prepared:\")\n",
    "print(f\"  Train examples: {len(dataset['train'])}\")\n",
    "print(f\"  Eval examples: {len(dataset['eval'])}\")\n",
    "\n",
    "# Inspect a sample\n",
    "print(\"\\nSample training example:\")\n",
    "print(dataset['train'][0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training arguments\n",
    "training_args = pipeline.setup_training_args()\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Output dir: {training_args.output_dir}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Weight decay: {training_args.weight_decay}\")\n",
    "print(f\"  Optimizer: {training_args.optim}\")\n",
    "print(f\"  BF16: {training_args.bf16}\")\n",
    "print(f\"  Gradient checkpointing: {training_args.gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\"*60)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "trainer = pipeline.train(dataset, training_args)\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"✓ Training completed in {duration}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"Running final evaluation...\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nEvaluation results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "with open(f\"{CONFIG['output_dir']}/eval_results.json\", \"w\") as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Evaluation results saved to {CONFIG['output_dir']}/eval_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model on a sample security finding\n",
    "\n",
    "test_finding = {\n",
    "    \"id\": \"test_001\",\n",
    "    \"title\": \"Suspicious process spawning shell\",\n",
    "    \"description\": \"Process /usr/bin/python3 spawned /bin/bash with unusual arguments\",\n",
    "    \"metadata\": {\n",
    "        \"pid\": 12345,\n",
    "        \"command\": \"/usr/bin/python3 -c 'import os; os.system(\\\"/bin/bash -i\\\")'\",\n",
    "        \"user\": \"www-data\",\n",
    "        \"ppid\": 1234\n",
    "    },\n",
    "    \"category\": \"processes\"\n",
    "}\n",
    "\n",
    "prompt = f\"\"\"[INST] Analyze this security finding and provide risk scoring with subscores (impact, exposure, anomaly, confidence), severity classification, and actionability assessment.\n",
    "\n",
    "Input:\n",
    "{json.dumps(test_finding, indent=2)}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(student_model.device)\n",
    "\n",
    "# Generate\n",
    "print(\"Generating response...\")\n",
    "with torch.no_grad():\n",
    "    outputs = student_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model Response:\")\n",
    "print(\"=\"*60)\n",
    "print(response)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "print(f\"Saving model to {CONFIG['output_dir']}...\")\n",
    "\n",
    "pipeline.save_model(trainer)\n",
    "\n",
    "print(\"\\n✓ Model saved successfully!\")\n",
    "print(f\"\\nModel location: {CONFIG['output_dir']}\")\n",
    "print(\"\\nTo use this model in sys-scan-graph:\")\n",
    "print(f\"1. Copy {CONFIG['output_dir']} to your sys-scan-graph agent directory\")\n",
    "print(\"2. Update sys_scan_agent/llm.py to load from this path\")\n",
    "print(\"3. Test with: sys-scan-graph analyze --report <report.json>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Upload to HuggingFace Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and configure to upload to HuggingFace\n",
    "\n",
    "\"\"\"\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# Login (you'll need a HuggingFace token)\n",
    "login()\n",
    "\n",
    "# Push to hub\n",
    "repo_name = \"your-username/mistral-7b-security-analyst\"\n",
    "\n",
    "student_model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"✓ Model uploaded to https://huggingface.co/{repo_name}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook fine-tuned a Mistral-7B model for security analysis tasks using:\n",
    "\n",
    "- **4-bit quantization** for memory efficiency\n",
    "- **LoRA adapters** for parameter-efficient fine-tuning\n",
    "- **Lion optimizer** for improved convergence\n",
    "- **Optional distillation** from teacher model\n",
    "- **Synthetic security data** generated from sys-scan patterns\n",
    "\n",
    "The resulting model can analyze security findings, score risks, identify correlations, and generate actionable recommendations - all locally without external API calls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
